{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# https://www.youtube.com/watch?v=ovB0ddFtzzA&t=876s\n",
    "\n",
    "class patchembed(nn.Module):\n",
    "    \"\"\" 원본이미지 -> 패치이미지로 만듬 패치 이미지 임베드\n",
    "    \n",
    "    Paramters\n",
    "    ---------\n",
    "    img_size : int\n",
    "        이미지의 사이즈 (정사각형)\n",
    "        변수값 들어갈때는 (img_size,img_size)로 들어감\n",
    "    \n",
    "    patch_size : int\n",
    "        패치가 될 사이즈\n",
    "        변수값 들어갈때는 (patch_size,patch_size)로 들어감\n",
    "    \n",
    "    int_chans : int\n",
    "        입력이미지 채널수\n",
    "        \n",
    "    embed_dim : int\n",
    "        임베딩할 차원\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self,img_size,patch_size,int_chans=3,embed_dim=768) -> None:\n",
    "        super().__init__()\n",
    "        self.img_size = img_size\n",
    "        self.patch_size = patch_size\n",
    "        \n",
    "        ## 패치 갯수\n",
    "        self.n_patches = (img_size // patch_size)**2\n",
    "\n",
    "        self.proj = nn.Conv2d(int_chans,embed_dim,kernel_size=patch_size,stride=patch_size)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        \"\"\" 피드포워드 계산\n",
    "        \n",
    "        Parameters\n",
    "        -----------\n",
    "        x : torch.Tensor\n",
    "            모양 '(배치,채널수,이미지사이즈,이미지사이즈)'\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        torch.tensor\n",
    "            모양 '(배치,패치갯수,임베딩 차원)'\n",
    "            \n",
    "        \"\"\"\n",
    "        \n",
    "        x = self.proj(x)\n",
    "        x = x.flatten(2) # (배치,임배딩차원수,패치수)\n",
    "        x = x.transpose(1,2) # (배치,패치수,임배딩차원수)\n",
    "        return x\n",
    "    \n",
    "    \n",
    "class Attention(nn.Module):\n",
    "    \"\"\" 어텐션 메커니즘\n",
    "    Parameters\n",
    "    ----------\n",
    "    dim : int\n",
    "        인풋 차원\n",
    "        \n",
    "    n_heads : int\n",
    "        어텐션 메카니즘 헤더 갯수\n",
    "\n",
    "    qkv_bias : bool\n",
    "        쿼리,키,벨류 바이어스 변수 설정할건지\n",
    "        \n",
    "    attn_p : float\n",
    "        드롭아웃 확률 (쿼리,키,벨류)\n",
    "    \n",
    "    proj_p : float\n",
    "        드롭아웃 확률 (출력 텐서)    \n",
    "    \n",
    "    \n",
    "    Attributes\n",
    "    ----------\n",
    "    scale : float\n",
    "        노멀라이징 \n",
    "    qkv : nn.Linear\n",
    "        키,쿼리,벨류\n",
    "        \n",
    "    proj : nn.Linear\n",
    "        어텐션 값들 덴스레이어\n",
    "        \n",
    "    attn_drop, proj_drop : nn.Dropout\n",
    "        드롭아웃 레이어    \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self,dim,n_heads=12,qkv_bias=True,attn_p=0.,proj_p=0.) -> None:\n",
    "        super().__init__()\n",
    "        self.n_heads = n_heads\n",
    "        self.dim = dim\n",
    "        self.head_dim = dim // n_heads # 멀티헤드 어텐션 헤드는... 인코더의 전체차원에서 n_heads만큼 나누어줌\n",
    "        self.scale = self.head_dim ** -0.5 ## 어텐션 벡터 스케일링\n",
    "        \n",
    "        \n",
    "        self.qkv = nn.Linear(dim,dim*3,bias=qkv_bias)\n",
    "        self.attn_drop = nn.Dropout(attn_p)\n",
    "        self.proj = nn.Linear(dim,dim) ## 멀티헤더 어텐션은 입력,출력 차원의 갯수는 똑같음\n",
    "        self.proj_drop = nn.Dropout(proj_p)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        \"\"\" 전방향 연산 시작, (멀티헤더 어텐션은 입력,출력 차원의 갯수는 똑같음)\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        x : torch.Tensor\n",
    "            모양 '(배치,패치수+1,dim)'\n",
    "            패치수+1은 앞에 클래스 토큰\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        torch.Tensor\n",
    "            모양 '(배치,패치수+1,dim)'\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        ## 배치수, 패치수, x의 차원\n",
    "        ## 여기서 패치수는 임베딩된 벡터라 하나의 토큰으로 보아도 무방함\n",
    "        n_samples, n_tokens, dim = x.shape\n",
    "        \n",
    "        \n",
    "        ## 멀티헤더 셀프 어텐션은 입력과 출력의 차원이 같아야하는데 맞지 않다면 오류임 \n",
    "        if dim != self.dim:\n",
    "            raise ValueError\n",
    "        \n",
    "        \n",
    "        ## qkv를 한꺼번에 계산 -> 리쉐이프\n",
    "        qkv = self.qkv(x) # (배치,패치+1,3*dim)\n",
    "        \n",
    "\n",
    "        qkv = qkv.reshape(n_samples,n_tokens,3,self.n_heads,self.head_dim) # (배치,패치수+1,3,해더수,해더 차원)\n",
    "        qkv = qkv.permute(2,0,3,1,4) # (3,배치,해더수,패치수+1,해더 차원)\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        ## 쿼리,키,벨류 값 가져오기\n",
    "        q,k,v = qkv[0],qkv[1],qkv[2]\n",
    "        \n",
    "        ## 키값 ??? \n",
    "        k_t = k.transpose(-2,-1) # (배치,해더수,해더차원,패치수+1)\n",
    "        \n",
    "        ## 두행렬을 곱하고 스케일 조정\n",
    "        dp = (q@k_t) * self.scale # (배치,해더수,패치수+1,패치수+1)\n",
    "        \n",
    "        \n",
    "        ## 어텐션 맵 만듬 (소프트 맥스 & 드롭아웃)\n",
    "        attn = dp.softmax(dim=-1)\n",
    "        attn = self.attn_drop(attn)\n",
    "        \n",
    "        \n",
    "        weighted_avg = attn @ v # (배치,해더수,패치수+1,해더차원)\n",
    "        weighted_avg = weighted_avg.transpose(1,2) # (배치,패치수+1,해더수,해더차원)\n",
    "        weighted_avg = weighted_avg.flatten(2) # (배치,패치수+1,)\n",
    "        \n",
    "        x = self.proj(weighted_avg)\n",
    "        x = self.proj_drop(x)\n",
    "        \n",
    "        \n",
    "        return x\n",
    "        \n",
    "class MLP(nn.Module):\n",
    "    \"\"\" 멀티 레이어\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    in_features: int\n",
    "        입력데이터 사이즈\n",
    "        \n",
    "    hidden_feactures : int\n",
    "        히든 레이어 갯수\n",
    "    \n",
    "    out_feactures : int\n",
    "        출력 사이즈\n",
    "    \n",
    "    p : float\n",
    "        드롭아웃 확률\n",
    "        \n",
    "    \"\"\"\n",
    "    def __init__(self,in_features,hidden_feactures,out_feactures,p=0.):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(in_features,hidden_feactures)\n",
    "        self.act = nn.GELU()\n",
    "        self.fc2 = nn.Linear(hidden_feactures,out_feactures)\n",
    "        self.drop = nn.Dropout(p)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.act(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.drop(x)\n",
    "        \n",
    "        return x\n",
    "        \n",
    "        \n",
    "        \n",
    "class Block(nn.Module):\n",
    "    \"\"\" 트랜스 포머 블럭\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    dim : int\n",
    "        임베딩 차원\n",
    "    \n",
    "    n_heads : int\n",
    "        어텐션 해더 갯수\n",
    "        \n",
    "    mlp_ratio : float\n",
    "        'dim'에 대한 'MLP' 모듈의 숨겨진 차원 크기를 결정\n",
    "    \n",
    "    qkv_bias : bool\n",
    "        키,쿼리,블럭 바이어스 변수 설정\n",
    "        \n",
    "    p, attn_p : float\n",
    "        드롭아웃 확률\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self,dim,n_heads,mlp_ratio=4.0,qkv_bias=True,p=0,attn_p=0):\n",
    "        super().__init__()\n",
    "        self.norm1 = nn.LayerNorm(dim, eps=1e-6)\n",
    "        self.attn = Attention(dim,n_heads=n_heads,qkv_bias=qkv_bias,attn_p=attn_p,proj_p=p)\n",
    "        self.norm2 = nn.LayerNorm(dim,eps=1e-6)\n",
    "        \n",
    "        ## MLP레이어 임베딩차원은 -> 트랜스포머의 출력 벡터의 4배로   \n",
    "        hidden_feactures = int(dim*mlp_ratio)\n",
    "        self.mlp = MLP(\n",
    "            in_features=dim,\n",
    "            hidden_feactures=hidden_feactures,\n",
    "            out_feactures=dim,\n",
    "        )\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x = x + self.attn(self.norm1(x))\n",
    "        x = x + self.mlp(self.norm2(x))\n",
    "        \n",
    "        return x\n",
    "        \n",
    "          \n",
    "class Vit(nn.Module):\n",
    "    def __init__(self,\n",
    "                 img_size=256,\n",
    "                 patch_size=16,\n",
    "                 in_chans=3,\n",
    "                 n_classes=1000,\n",
    "                 embed_dim=768,\n",
    "                 depth=1,\n",
    "                 n_heads=12,\n",
    "                 mlp_ratio=4.,\n",
    "                 qkv_bias=False,\n",
    "                 p=0.,\n",
    "                 attn_p=0.,                 \n",
    "                 ):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.patch_embed = patchembed(\n",
    "            img_size=img_size,\n",
    "            patch_size=patch_size,\n",
    "            int_chans=in_chans,\n",
    "            embed_dim=embed_dim\n",
    "        )\n",
    "        \n",
    "        ## 임베드 벡터의 맨앞에 붙일 클래스 토큰\n",
    "        self.cls_token = nn.Parameter(torch.zeros(1,1,embed_dim))\n",
    "        \n",
    "        ## 포지션 파라미터들\n",
    "        self.pos_embed = nn.Parameter(torch.zeros(1,1+self.patch_embed.n_patches,embed_dim))\n",
    "        \n",
    "        self.pos_drop = nn.Dropout(p=p)\n",
    "        \n",
    "        self.blocks = nn.ModuleList(\n",
    "            [\n",
    "                Block(\n",
    "                    dim = embed_dim,\n",
    "                    n_heads=n_heads,\n",
    "                    mlp_ratio=mlp_ratio,\n",
    "                    qkv_bias=qkv_bias,\n",
    "                    p=p,\n",
    "                    attn_p=attn_p,                    \n",
    "                )\n",
    "                for _ in range(depth)\n",
    "            ]\n",
    "        )\n",
    "        \n",
    "        self.norm = nn.LayerNorm(embed_dim,eps=1e-6)\n",
    "        self.head = nn.Linear(embed_dim,n_classes)\n",
    "    \n",
    "    def forward(self,x):\n",
    "        ## 배치수\n",
    "        n_samples = x.shape[0]\n",
    "        x = self.patch_embed(x)\n",
    "        \n",
    "        cls_token = self.cls_token.expand(n_samples,-1,-1) # (배치,1,임베드차원)\n",
    "        \n",
    "        ## cls 토큰을 붙임\n",
    "        x = torch.cat((cls_token,x),dim=1)\n",
    "        \n",
    "        x = x + self.pos_embed # (qocl,1+패치수,임베딩차원)\n",
    "        x = self.pos_drop(x)\n",
    "        \n",
    "        for block in self.blocks:\n",
    "            x = block(x)\n",
    "        \n",
    "        x = self.norm(x)\n",
    "        \n",
    "        cls_token_final = x[:,0] # vit 마지막 결과값 가져옴\n",
    "        x = self.head(cls_token_final)\n",
    "        \n",
    "        return x  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "## 데이터 로더\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "img_size = 256\n",
    "batch_size = 4\n",
    "\n",
    "mean = (0.4914, 0.4822, 0.4465)\n",
    "std = (0.2023, 0.1994, 0.2010)\n",
    "train_transform = transforms.Compose([transforms.Resize(img_size), transforms.RandomCrop(img_size, padding=2),\n",
    "                                        transforms.RandomHorizontalFlip(), transforms.ToTensor(), transforms.Normalize(mean, std)])\n",
    "test_transform = transforms.Compose([transforms.Resize(img_size), transforms.ToTensor(),\n",
    "                                        transforms.Normalize(mean, std)])\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=train_transform)\n",
    "valset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=test_transform)\n",
    "\n",
    "\n",
    "trainloader = DataLoader(trainset, batch_size=batch_size)\n",
    "valloader = DataLoader(valset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Vit(\n",
       "  (patch_embed): patchembed(\n",
       "    (proj): Conv2d(3, 512, kernel_size=(4, 4), stride=(4, 4))\n",
       "  )\n",
       "  (pos_drop): Dropout(p=0.0, inplace=False)\n",
       "  (blocks): ModuleList(\n",
       "    (0): Block(\n",
       "      (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=512, out_features=1536, bias=False)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): MLP(\n",
       "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "        (act): GELU()\n",
       "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        (drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
       "  (head): Linear(in_features=512, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 모델 선언\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = Vit(img_size=img_size,patch_size=4,in_chans=3,n_classes=10,embed_dim=512,n_heads=8,depth=4)\n",
    "\n",
    "# model = Vit(img_size=img_size,patch_size=4,in_chans=3,n_classes=10)\n",
    "\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0   12500/0] 에폭 평균 loss : 0.5630 배치 loss : 2.2521 accuracy : 1/4\n",
      "[0   12500/100] 에폭 평균 loss : 0.6118 배치 loss : 2.9361 accuracy : 1/4\n",
      "[0   12500/200] 에폭 평균 loss : 0.6046 배치 loss : 2.8324 accuracy : 0/4\n",
      "[0   12500/300] 에폭 평균 loss : 0.6092 배치 loss : 2.1959 accuracy : 0/4\n",
      "[0   12500/400] 에폭 평균 loss : 0.6081 배치 loss : 2.1784 accuracy : 0/4\n",
      "[0   12500/500] 에폭 평균 loss : 0.6070 배치 loss : 2.0744 accuracy : 0/4\n",
      "[0   12500/600] 에폭 평균 loss : 0.6066 배치 loss : 2.5369 accuracy : 0/4\n",
      "[0   12500/700] 에폭 평균 loss : 0.6072 배치 loss : 2.5621 accuracy : 0/4\n",
      "[0   12500/800] 에폭 평균 loss : 0.6076 배치 loss : 2.6209 accuracy : 0/4\n",
      "[0   12500/900] 에폭 평균 loss : 0.6058 배치 loss : 2.0270 accuracy : 2/4\n",
      "[0   12500/1000] 에폭 평균 loss : 0.6050 배치 loss : 2.4233 accuracy : 0/4\n",
      "[0   12500/1100] 에폭 평균 loss : 0.6066 배치 loss : 2.7650 accuracy : 0/4\n",
      "[0   12500/1200] 에폭 평균 loss : 0.6067 배치 loss : 2.4166 accuracy : 0/4\n",
      "[0   12500/1300] 에폭 평균 loss : 0.6061 배치 loss : 2.4852 accuracy : 1/4\n",
      "[0   12500/1400] 에폭 평균 loss : 0.6063 배치 loss : 2.3186 accuracy : 0/4\n",
      "[0   12500/1500] 에폭 평균 loss : 0.6061 배치 loss : 2.3568 accuracy : 1/4\n",
      "[0   12500/1600] 에폭 평균 loss : 0.6054 배치 loss : 2.3120 accuracy : 0/4\n",
      "[0   12500/1700] 에폭 평균 loss : 0.6054 배치 loss : 2.9286 accuracy : 0/4\n",
      "[0   12500/1800] 에폭 평균 loss : 0.6059 배치 loss : 2.9485 accuracy : 0/4\n",
      "[0   12500/1900] 에폭 평균 loss : 0.6068 배치 loss : 2.5399 accuracy : 0/4\n",
      "[0   12500/2000] 에폭 평균 loss : 0.6070 배치 loss : 2.5013 accuracy : 0/4\n",
      "[0   12500/2100] 에폭 평균 loss : 0.6075 배치 loss : 2.3502 accuracy : 0/4\n",
      "[0   12500/2200] 에폭 평균 loss : 0.6076 배치 loss : 2.5246 accuracy : 0/4\n",
      "[0   12500/2300] 에폭 평균 loss : 0.6076 배치 loss : 2.3027 accuracy : 1/4\n",
      "[0   12500/2400] 에폭 평균 loss : 0.6079 배치 loss : 2.2638 accuracy : 1/4\n",
      "[0   12500/2500] 에폭 평균 loss : 0.6082 배치 loss : 2.5225 accuracy : 0/4\n",
      "[0   12500/2600] 에폭 평균 loss : 0.6083 배치 loss : 2.5793 accuracy : 0/4\n",
      "[0   12500/2700] 에폭 평균 loss : 0.6084 배치 loss : 2.5121 accuracy : 1/4\n",
      "[0   12500/2800] 에폭 평균 loss : 0.6083 배치 loss : 2.4314 accuracy : 0/4\n",
      "[0   12500/2900] 에폭 평균 loss : 0.6083 배치 loss : 2.0859 accuracy : 1/4\n",
      "[0   12500/3000] 에폭 평균 loss : 0.6085 배치 loss : 2.7884 accuracy : 0/4\n",
      "[0   12500/3100] 에폭 평균 loss : 0.6087 배치 loss : 2.2975 accuracy : 0/4\n",
      "[0   12500/3200] 에폭 평균 loss : 0.6087 배치 loss : 2.2923 accuracy : 1/4\n",
      "[0   12500/3300] 에폭 평균 loss : 0.6090 배치 loss : 2.4941 accuracy : 0/4\n",
      "[0   12500/3400] 에폭 평균 loss : 0.6094 배치 loss : 3.3100 accuracy : 0/4\n",
      "[0   12500/3500] 에폭 평균 loss : 0.6094 배치 loss : 2.4605 accuracy : 0/4\n",
      "[0   12500/3600] 에폭 평균 loss : 0.6094 배치 loss : 2.3135 accuracy : 1/4\n",
      "[0   12500/3700] 에폭 평균 loss : 0.6092 배치 loss : 2.2825 accuracy : 1/4\n",
      "[0   12500/3800] 에폭 평균 loss : 0.6094 배치 loss : 1.9339 accuracy : 2/4\n",
      "[0   12500/3900] 에폭 평균 loss : 0.6096 배치 loss : 2.5071 accuracy : 1/4\n",
      "[0   12500/4000] 에폭 평균 loss : 0.6097 배치 loss : 2.3882 accuracy : 1/4\n",
      "[0   12500/4100] 에폭 평균 loss : 0.6094 배치 loss : 2.2249 accuracy : 1/4\n",
      "[0   12500/4200] 에폭 평균 loss : 0.6093 배치 loss : 2.2093 accuracy : 0/4\n",
      "[0   12500/4300] 에폭 평균 loss : 0.6094 배치 loss : 2.4763 accuracy : 0/4\n",
      "[0   12500/4400] 에폭 평균 loss : 0.6095 배치 loss : 2.7188 accuracy : 1/4\n",
      "[0   12500/4500] 에폭 평균 loss : 0.6098 배치 loss : 2.2475 accuracy : 1/4\n",
      "[0   12500/4600] 에폭 평균 loss : 0.6098 배치 loss : 2.3826 accuracy : 0/4\n",
      "[0   12500/4700] 에폭 평균 loss : 0.6099 배치 loss : 2.2312 accuracy : 1/4\n",
      "[0   12500/4800] 에폭 평균 loss : 0.6100 배치 loss : 2.6318 accuracy : 0/4\n",
      "[0   12500/4900] 에폭 평균 loss : 0.6099 배치 loss : 2.3551 accuracy : 0/4\n",
      "[0   12500/5000] 에폭 평균 loss : 0.6100 배치 loss : 2.8827 accuracy : 0/4\n",
      "[0   12500/5100] 에폭 평균 loss : 0.6100 배치 loss : 2.7584 accuracy : 0/4\n",
      "[0   12500/5200] 에폭 평균 loss : 0.6098 배치 loss : 2.5695 accuracy : 0/4\n",
      "[0   12500/5300] 에폭 평균 loss : 0.6098 배치 loss : 2.1827 accuracy : 0/4\n",
      "[0   12500/5400] 에폭 평균 loss : 0.6097 배치 loss : 2.5499 accuracy : 0/4\n",
      "[0   12500/5500] 에폭 평균 loss : 0.6096 배치 loss : 2.6193 accuracy : 0/4\n",
      "[0   12500/5600] 에폭 평균 loss : 0.6097 배치 loss : 2.5013 accuracy : 0/4\n",
      "[0   12500/5700] 에폭 평균 loss : 0.6097 배치 loss : 2.4305 accuracy : 0/4\n",
      "[0   12500/5800] 에폭 평균 loss : 0.6098 배치 loss : 2.3610 accuracy : 0/4\n",
      "[0   12500/5900] 에폭 평균 loss : 0.6098 배치 loss : 2.1928 accuracy : 1/4\n",
      "[0   12500/6000] 에폭 평균 loss : 0.6097 배치 loss : 2.8884 accuracy : 1/4\n",
      "[0   12500/6100] 에폭 평균 loss : 0.6097 배치 loss : 2.7889 accuracy : 0/4\n",
      "[0   12500/6200] 에폭 평균 loss : 0.6095 배치 loss : 2.8101 accuracy : 0/4\n",
      "[0   12500/6300] 에폭 평균 loss : 0.6096 배치 loss : 2.8712 accuracy : 0/4\n",
      "[0   12500/6400] 에폭 평균 loss : 0.6096 배치 loss : 2.4391 accuracy : 1/4\n",
      "[0   12500/6500] 에폭 평균 loss : 0.6097 배치 loss : 2.5028 accuracy : 0/4\n",
      "[0   12500/6600] 에폭 평균 loss : 0.6097 배치 loss : 2.2221 accuracy : 1/4\n",
      "[0   12500/6700] 에폭 평균 loss : 0.6096 배치 loss : 1.7708 accuracy : 1/4\n",
      "[0   12500/6800] 에폭 평균 loss : 0.6095 배치 loss : 2.5627 accuracy : 0/4\n",
      "[0   12500/6900] 에폭 평균 loss : 0.6093 배치 loss : 2.3281 accuracy : 0/4\n",
      "[0   12500/7000] 에폭 평균 loss : 0.6093 배치 loss : 2.5209 accuracy : 0/4\n",
      "[0   12500/7100] 에폭 평균 loss : 0.6093 배치 loss : 2.1718 accuracy : 1/4\n",
      "[0   12500/7200] 에폭 평균 loss : 0.6093 배치 loss : 2.3627 accuracy : 1/4\n",
      "[0   12500/7300] 에폭 평균 loss : 0.6092 배치 loss : 2.3912 accuracy : 1/4\n",
      "[0   12500/7400] 에폭 평균 loss : 0.6093 배치 loss : 2.4249 accuracy : 0/4\n",
      "[0   12500/7500] 에폭 평균 loss : 0.6093 배치 loss : 2.1736 accuracy : 0/4\n",
      "[0   12500/7600] 에폭 평균 loss : 0.6093 배치 loss : 2.7374 accuracy : 1/4\n",
      "[0   12500/7700] 에폭 평균 loss : 0.6093 배치 loss : 2.7097 accuracy : 0/4\n",
      "[0   12500/7800] 에폭 평균 loss : 0.6093 배치 loss : 2.1702 accuracy : 0/4\n",
      "[0   12500/7900] 에폭 평균 loss : 0.6094 배치 loss : 2.9664 accuracy : 0/4\n",
      "[0   12500/8000] 에폭 평균 loss : 0.6093 배치 loss : 2.1743 accuracy : 0/4\n",
      "[0   12500/8100] 에폭 평균 loss : 0.6094 배치 loss : 2.7935 accuracy : 0/4\n",
      "[0   12500/8200] 에폭 평균 loss : 0.6093 배치 loss : 2.5169 accuracy : 0/4\n",
      "[0   12500/8300] 에폭 평균 loss : 0.6093 배치 loss : 2.8414 accuracy : 0/4\n",
      "[0   12500/8400] 에폭 평균 loss : 0.6095 배치 loss : 2.5696 accuracy : 1/4\n",
      "[0   12500/8500] 에폭 평균 loss : 0.6095 배치 loss : 2.3775 accuracy : 0/4\n",
      "[0   12500/8600] 에폭 평균 loss : 0.6095 배치 loss : 2.3593 accuracy : 0/4\n",
      "[0   12500/8700] 에폭 평균 loss : 0.6095 배치 loss : 2.2392 accuracy : 1/4\n",
      "[0   12500/8800] 에폭 평균 loss : 0.6095 배치 loss : 2.3053 accuracy : 0/4\n",
      "[0   12500/8900] 에폭 평균 loss : 0.6094 배치 loss : 2.5996 accuracy : 0/4\n",
      "[0   12500/9000] 에폭 평균 loss : 0.6095 배치 loss : 2.4968 accuracy : 0/4\n",
      "[0   12500/9100] 에폭 평균 loss : 0.6095 배치 loss : 2.2263 accuracy : 2/4\n",
      "[0   12500/9200] 에폭 평균 loss : 0.6095 배치 loss : 2.5317 accuracy : 0/4\n",
      "[0   12500/9300] 에폭 평균 loss : 0.6095 배치 loss : 2.3970 accuracy : 0/4\n",
      "[0   12500/9400] 에폭 평균 loss : 0.6094 배치 loss : 2.3243 accuracy : 1/4\n",
      "[0   12500/9500] 에폭 평균 loss : 0.6095 배치 loss : 2.6207 accuracy : 0/4\n",
      "[0   12500/9600] 에폭 평균 loss : 0.6095 배치 loss : 2.1439 accuracy : 1/4\n",
      "[0   12500/9700] 에폭 평균 loss : 0.6096 배치 loss : 2.2094 accuracy : 1/4\n",
      "[0   12500/9800] 에폭 평균 loss : 0.6096 배치 loss : 2.6482 accuracy : 0/4\n",
      "[0   12500/9900] 에폭 평균 loss : 0.6096 배치 loss : 2.1509 accuracy : 1/4\n",
      "[0   12500/10000] 에폭 평균 loss : 0.6095 배치 loss : 2.3938 accuracy : 1/4\n",
      "[0   12500/10100] 에폭 평균 loss : 0.6094 배치 loss : 2.4172 accuracy : 1/4\n",
      "[0   12500/10200] 에폭 평균 loss : 0.6095 배치 loss : 2.1051 accuracy : 1/4\n",
      "[0   12500/10300] 에폭 평균 loss : 0.6095 배치 loss : 2.0946 accuracy : 0/4\n",
      "[0   12500/10400] 에폭 평균 loss : 0.6096 배치 loss : 2.8528 accuracy : 0/4\n",
      "[0   12500/10500] 에폭 평균 loss : 0.6098 배치 loss : 2.2130 accuracy : 0/4\n",
      "[0   12500/10600] 에폭 평균 loss : 0.6097 배치 loss : 2.2030 accuracy : 0/4\n",
      "[0   12500/10700] 에폭 평균 loss : 0.6096 배치 loss : 2.1855 accuracy : 1/4\n",
      "[0   12500/10800] 에폭 평균 loss : 0.6096 배치 loss : 2.3479 accuracy : 0/4\n",
      "[0   12500/10900] 에폭 평균 loss : 0.6096 배치 loss : 2.5841 accuracy : 1/4\n",
      "[0   12500/11000] 에폭 평균 loss : 0.6096 배치 loss : 2.0586 accuracy : 0/4\n",
      "[0   12500/11100] 에폭 평균 loss : 0.6096 배치 loss : 1.9491 accuracy : 1/4\n",
      "[0   12500/11200] 에폭 평균 loss : 0.6096 배치 loss : 2.7287 accuracy : 0/4\n",
      "[0   12500/11300] 에폭 평균 loss : 0.6095 배치 loss : 2.4822 accuracy : 1/4\n",
      "[0   12500/11400] 에폭 평균 loss : 0.6096 배치 loss : 2.4809 accuracy : 0/4\n",
      "[0   12500/11500] 에폭 평균 loss : 0.6096 배치 loss : 2.3558 accuracy : 0/4\n",
      "[0   12500/11600] 에폭 평균 loss : 0.6097 배치 loss : 2.3678 accuracy : 0/4\n",
      "[0   12500/11700] 에폭 평균 loss : 0.6096 배치 loss : 2.3996 accuracy : 0/4\n",
      "[0   12500/11800] 에폭 평균 loss : 0.6095 배치 loss : 2.7517 accuracy : 0/4\n",
      "[0   12500/11900] 에폭 평균 loss : 0.6096 배치 loss : 2.5319 accuracy : 0/4\n",
      "[0   12500/12000] 에폭 평균 loss : 0.6097 배치 loss : 2.7199 accuracy : 1/4\n",
      "[0   12500/12100] 에폭 평균 loss : 0.6096 배치 loss : 2.6506 accuracy : 0/4\n",
      "[0   12500/12200] 에폭 평균 loss : 0.6096 배치 loss : 2.5413 accuracy : 1/4\n",
      "[0   12500/12300] 에폭 평균 loss : 0.6096 배치 loss : 2.4385 accuracy : 0/4\n",
      "[0   12500/12400] 에폭 평균 loss : 0.6097 배치 loss : 2.4371 accuracy : 1/4\n",
      "에폭 끝.... 평균 loss : 7620.499714165926\n",
      "[1   12500/0] 에폭 평균 loss : 0.5631 배치 loss : 2.2524 accuracy : 1/4\n",
      "[1   12500/100] 에폭 평균 loss : 0.6116 배치 loss : 2.9283 accuracy : 1/4\n",
      "[1   12500/200] 에폭 평균 loss : 0.6047 배치 loss : 2.8508 accuracy : 0/4\n",
      "[1   12500/300] 에폭 평균 loss : 0.6093 배치 loss : 2.2115 accuracy : 0/4\n",
      "[1   12500/400] 에폭 평균 loss : 0.6082 배치 loss : 2.1720 accuracy : 0/4\n",
      "[1   12500/500] 에폭 평균 loss : 0.6071 배치 loss : 2.1001 accuracy : 0/4\n",
      "[1   12500/600] 에폭 평균 loss : 0.6068 배치 loss : 2.5282 accuracy : 0/4\n",
      "[1   12500/700] 에폭 평균 loss : 0.6074 배치 loss : 2.5359 accuracy : 0/4\n",
      "[1   12500/800] 에폭 평균 loss : 0.6077 배치 loss : 2.6222 accuracy : 0/4\n",
      "[1   12500/900] 에폭 평균 loss : 0.6059 배치 loss : 2.0253 accuracy : 2/4\n",
      "[1   12500/1000] 에폭 평균 loss : 0.6051 배치 loss : 2.4244 accuracy : 0/4\n",
      "[1   12500/1100] 에폭 평균 loss : 0.6066 배치 loss : 2.7664 accuracy : 0/4\n",
      "[1   12500/1200] 에폭 평균 loss : 0.6067 배치 loss : 2.4136 accuracy : 0/4\n",
      "[1   12500/1300] 에폭 평균 loss : 0.6061 배치 loss : 2.4765 accuracy : 1/4\n",
      "[1   12500/1400] 에폭 평균 loss : 0.6063 배치 loss : 2.3186 accuracy : 0/4\n",
      "[1   12500/1500] 에폭 평균 loss : 0.6061 배치 loss : 2.3665 accuracy : 1/4\n",
      "[1   12500/1600] 에폭 평균 loss : 0.6054 배치 loss : 2.3073 accuracy : 0/4\n",
      "[1   12500/1700] 에폭 평균 loss : 0.6054 배치 loss : 2.8853 accuracy : 0/4\n",
      "[1   12500/1800] 에폭 평균 loss : 0.6059 배치 loss : 2.9443 accuracy : 0/4\n",
      "[1   12500/1900] 에폭 평균 loss : 0.6068 배치 loss : 2.5300 accuracy : 0/4\n",
      "[1   12500/2000] 에폭 평균 loss : 0.6070 배치 loss : 2.5034 accuracy : 0/4\n",
      "[1   12500/2100] 에폭 평균 loss : 0.6075 배치 loss : 2.3566 accuracy : 0/4\n",
      "[1   12500/2200] 에폭 평균 loss : 0.6075 배치 loss : 2.5199 accuracy : 0/4\n",
      "[1   12500/2300] 에폭 평균 loss : 0.6076 배치 loss : 2.3001 accuracy : 0/4\n",
      "[1   12500/2400] 에폭 평균 loss : 0.6079 배치 loss : 2.2555 accuracy : 1/4\n",
      "[1   12500/2500] 에폭 평균 loss : 0.6082 배치 loss : 2.5243 accuracy : 0/4\n",
      "[1   12500/2600] 에폭 평균 loss : 0.6082 배치 loss : 2.5879 accuracy : 0/4\n",
      "[1   12500/2700] 에폭 평균 loss : 0.6084 배치 loss : 2.5224 accuracy : 1/4\n",
      "[1   12500/2800] 에폭 평균 loss : 0.6082 배치 loss : 2.4400 accuracy : 0/4\n",
      "[1   12500/2900] 에폭 평균 loss : 0.6083 배치 loss : 2.0876 accuracy : 1/4\n",
      "[1   12500/3000] 에폭 평균 loss : 0.6085 배치 loss : 2.7710 accuracy : 0/4\n",
      "[1   12500/3100] 에폭 평균 loss : 0.6087 배치 loss : 2.2701 accuracy : 0/4\n",
      "[1   12500/3200] 에폭 평균 loss : 0.6087 배치 loss : 2.3078 accuracy : 1/4\n",
      "[1   12500/3300] 에폭 평균 loss : 0.6089 배치 loss : 2.5235 accuracy : 0/4\n",
      "[1   12500/3400] 에폭 평균 loss : 0.6093 배치 loss : 3.3253 accuracy : 0/4\n",
      "[1   12500/3500] 에폭 평균 loss : 0.6094 배치 loss : 2.4653 accuracy : 0/4\n",
      "[1   12500/3600] 에폭 평균 loss : 0.6093 배치 loss : 2.3015 accuracy : 1/4\n",
      "[1   12500/3700] 에폭 평균 loss : 0.6092 배치 loss : 2.2832 accuracy : 1/4\n",
      "[1   12500/3800] 에폭 평균 loss : 0.6094 배치 loss : 1.9646 accuracy : 2/4\n",
      "[1   12500/3900] 에폭 평균 loss : 0.6096 배치 loss : 2.5409 accuracy : 1/4\n",
      "[1   12500/4000] 에폭 평균 loss : 0.6096 배치 loss : 2.3845 accuracy : 1/4\n",
      "[1   12500/4100] 에폭 평균 loss : 0.6094 배치 loss : 2.2260 accuracy : 1/4\n",
      "[1   12500/4200] 에폭 평균 loss : 0.6093 배치 loss : 2.2143 accuracy : 0/4\n",
      "[1   12500/4300] 에폭 평균 loss : 0.6094 배치 loss : 2.4699 accuracy : 0/4\n",
      "[1   12500/4400] 에폭 평균 loss : 0.6094 배치 loss : 2.7264 accuracy : 1/4\n",
      "[1   12500/4500] 에폭 평균 loss : 0.6097 배치 loss : 2.2492 accuracy : 1/4\n",
      "[1   12500/4600] 에폭 평균 loss : 0.6098 배치 loss : 2.3886 accuracy : 0/4\n",
      "[1   12500/4700] 에폭 평균 loss : 0.6099 배치 loss : 2.2207 accuracy : 0/4\n",
      "[1   12500/4800] 에폭 평균 loss : 0.6100 배치 loss : 2.6393 accuracy : 0/4\n",
      "[1   12500/4900] 에폭 평균 loss : 0.6099 배치 loss : 2.3381 accuracy : 0/4\n",
      "[1   12500/5000] 에폭 평균 loss : 0.6100 배치 loss : 2.8695 accuracy : 0/4\n",
      "[1   12500/5100] 에폭 평균 loss : 0.6100 배치 loss : 2.7751 accuracy : 0/4\n",
      "[1   12500/5200] 에폭 평균 loss : 0.6098 배치 loss : 2.5826 accuracy : 0/4\n",
      "[1   12500/5300] 에폭 평균 loss : 0.6098 배치 loss : 2.1812 accuracy : 0/4\n",
      "[1   12500/5400] 에폭 평균 loss : 0.6097 배치 loss : 2.5601 accuracy : 0/4\n",
      "[1   12500/5500] 에폭 평균 loss : 0.6096 배치 loss : 2.6256 accuracy : 0/4\n",
      "[1   12500/5600] 에폭 평균 loss : 0.6097 배치 loss : 2.5069 accuracy : 0/4\n",
      "[1   12500/5700] 에폭 평균 loss : 0.6097 배치 loss : 2.4209 accuracy : 1/4\n",
      "[1   12500/5800] 에폭 평균 loss : 0.6098 배치 loss : 2.3622 accuracy : 0/4\n",
      "[1   12500/5900] 에폭 평균 loss : 0.6098 배치 loss : 2.1798 accuracy : 1/4\n",
      "[1   12500/6000] 에폭 평균 loss : 0.6097 배치 loss : 2.9025 accuracy : 1/4\n",
      "[1   12500/6100] 에폭 평균 loss : 0.6097 배치 loss : 2.7897 accuracy : 0/4\n",
      "[1   12500/6200] 에폭 평균 loss : 0.6095 배치 loss : 2.8252 accuracy : 0/4\n",
      "[1   12500/6300] 에폭 평균 loss : 0.6096 배치 loss : 2.8800 accuracy : 0/4\n",
      "[1   12500/6400] 에폭 평균 loss : 0.6096 배치 loss : 2.4532 accuracy : 1/4\n",
      "[1   12500/6500] 에폭 평균 loss : 0.6097 배치 loss : 2.4999 accuracy : 0/4\n",
      "[1   12500/6600] 에폭 평균 loss : 0.6097 배치 loss : 2.2183 accuracy : 1/4\n",
      "[1   12500/6700] 에폭 평균 loss : 0.6095 배치 loss : 1.7908 accuracy : 1/4\n",
      "[1   12500/6800] 에폭 평균 loss : 0.6095 배치 loss : 2.5741 accuracy : 0/4\n",
      "[1   12500/6900] 에폭 평균 loss : 0.6093 배치 loss : 2.3421 accuracy : 0/4\n",
      "[1   12500/7000] 에폭 평균 loss : 0.6093 배치 loss : 2.5268 accuracy : 0/4\n",
      "[1   12500/7100] 에폭 평균 loss : 0.6093 배치 loss : 2.1635 accuracy : 1/4\n",
      "[1   12500/7200] 에폭 평균 loss : 0.6093 배치 loss : 2.3509 accuracy : 1/4\n",
      "[1   12500/7300] 에폭 평균 loss : 0.6092 배치 loss : 2.3514 accuracy : 1/4\n",
      "[1   12500/7400] 에폭 평균 loss : 0.6092 배치 loss : 2.4449 accuracy : 0/4\n",
      "[1   12500/7500] 에폭 평균 loss : 0.6093 배치 loss : 2.1784 accuracy : 0/4\n",
      "[1   12500/7600] 에폭 평균 loss : 0.6093 배치 loss : 2.7230 accuracy : 1/4\n",
      "[1   12500/7700] 에폭 평균 loss : 0.6093 배치 loss : 2.7185 accuracy : 0/4\n",
      "[1   12500/7800] 에폭 평균 loss : 0.6093 배치 loss : 2.1575 accuracy : 0/4\n",
      "[1   12500/7900] 에폭 평균 loss : 0.6094 배치 loss : 2.9910 accuracy : 0/4\n",
      "[1   12500/8000] 에폭 평균 loss : 0.6093 배치 loss : 2.1469 accuracy : 0/4\n",
      "[1   12500/8100] 에폭 평균 loss : 0.6094 배치 loss : 2.7959 accuracy : 0/4\n",
      "[1   12500/8200] 에폭 평균 loss : 0.6093 배치 loss : 2.5148 accuracy : 0/4\n",
      "[1   12500/8300] 에폭 평균 loss : 0.6093 배치 loss : 2.8293 accuracy : 0/4\n",
      "[1   12500/8400] 에폭 평균 loss : 0.6095 배치 loss : 2.5527 accuracy : 1/4\n",
      "[1   12500/8500] 에폭 평균 loss : 0.6095 배치 loss : 2.3696 accuracy : 0/4\n",
      "[1   12500/8600] 에폭 평균 loss : 0.6095 배치 loss : 2.3474 accuracy : 0/4\n",
      "[1   12500/8700] 에폭 평균 loss : 0.6095 배치 loss : 2.2385 accuracy : 1/4\n",
      "[1   12500/8800] 에폭 평균 loss : 0.6094 배치 loss : 2.3001 accuracy : 0/4\n",
      "[1   12500/8900] 에폭 평균 loss : 0.6094 배치 loss : 2.6044 accuracy : 0/4\n",
      "[1   12500/9000] 에폭 평균 loss : 0.6095 배치 loss : 2.5013 accuracy : 0/4\n",
      "[1   12500/9100] 에폭 평균 loss : 0.6095 배치 loss : 2.2232 accuracy : 2/4\n",
      "[1   12500/9200] 에폭 평균 loss : 0.6095 배치 loss : 2.5171 accuracy : 0/4\n",
      "[1   12500/9300] 에폭 평균 loss : 0.6095 배치 loss : 2.3724 accuracy : 0/4\n",
      "[1   12500/9400] 에폭 평균 loss : 0.6094 배치 loss : 2.3185 accuracy : 1/4\n",
      "[1   12500/9500] 에폭 평균 loss : 0.6095 배치 loss : 2.6189 accuracy : 0/4\n",
      "[1   12500/9600] 에폭 평균 loss : 0.6095 배치 loss : 2.1389 accuracy : 1/4\n",
      "[1   12500/9700] 에폭 평균 loss : 0.6095 배치 loss : 2.1903 accuracy : 1/4\n",
      "[1   12500/9800] 에폭 평균 loss : 0.6095 배치 loss : 2.6686 accuracy : 0/4\n",
      "[1   12500/9900] 에폭 평균 loss : 0.6096 배치 loss : 2.1456 accuracy : 1/4\n",
      "[1   12500/10000] 에폭 평균 loss : 0.6095 배치 loss : 2.3770 accuracy : 1/4\n",
      "[1   12500/10100] 에폭 평균 loss : 0.6094 배치 loss : 2.4087 accuracy : 1/4\n",
      "[1   12500/10200] 에폭 평균 loss : 0.6094 배치 loss : 2.1078 accuracy : 1/4\n",
      "[1   12500/10300] 에폭 평균 loss : 0.6095 배치 loss : 2.0905 accuracy : 0/4\n",
      "[1   12500/10400] 에폭 평균 loss : 0.6096 배치 loss : 2.8573 accuracy : 0/4\n",
      "[1   12500/10500] 에폭 평균 loss : 0.6097 배치 loss : 2.1922 accuracy : 0/4\n",
      "[1   12500/10600] 에폭 평균 loss : 0.6097 배치 loss : 2.2027 accuracy : 1/4\n",
      "[1   12500/10700] 에폭 평균 loss : 0.6096 배치 loss : 2.1430 accuracy : 1/4\n",
      "[1   12500/10800] 에폭 평균 loss : 0.6096 배치 loss : 2.3650 accuracy : 0/4\n",
      "[1   12500/10900] 에폭 평균 loss : 0.6096 배치 loss : 2.5765 accuracy : 1/4\n",
      "[1   12500/11000] 에폭 평균 loss : 0.6096 배치 loss : 2.0596 accuracy : 0/4\n",
      "[1   12500/11100] 에폭 평균 loss : 0.6096 배치 loss : 1.9567 accuracy : 1/4\n",
      "[1   12500/11200] 에폭 평균 loss : 0.6096 배치 loss : 2.7238 accuracy : 0/4\n",
      "[1   12500/11300] 에폭 평균 loss : 0.6095 배치 loss : 2.4949 accuracy : 1/4\n"
     ]
    }
   ],
   "source": [
    "# 학습\n",
    "import torch.optim as optim\n",
    "epochs = 200\n",
    "lr = 0.001\n",
    "weight_decay = 0\n",
    "\n",
    "interver=100\n",
    "\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "for epoch in range(0,epochs):\n",
    "    \n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    running_loss = 0\n",
    "    \n",
    "    for i,(img,target) in enumerate(trainloader):\n",
    "        outputs = model(img.to(device))\n",
    "        loss = criterion(outputs, target.to(device))\n",
    "        loss.backward()\n",
    "        \n",
    "        running_loss += loss.item()        \n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        correct = (predicted == target.to(device)).sum().item() \n",
    "        \n",
    "        epoch_loss = running_loss / (batch_size*(i+1))\n",
    "        \n",
    "        if i % interver ==0:\n",
    "            print(f'[{epoch}   {len(trainloader)}/{i}] 에폭 평균 loss : {epoch_loss:.4f} 배치 loss : {loss:.4f} accuracy : {correct}/{batch_size}')    \n",
    "        \n",
    "    train_epoch_loss_avg = running_loss / batch_size\n",
    "    print(f\"에폭 끝.... 평균 loss : {train_epoch_loss_avg}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 넣기전 torch.Size([2, 4097, 768])\n",
    "# 2 4097 3 12 21\n",
    "# # (배치,패치수+1,3,해더수,해더 차원)\n",
    "\n",
    "# shape '[2, 4097, 3, 12, 21]' is invalid for input of size 6292992\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "5320f61119de878fb31c813ca30206b3db486be99fef9b78f5991d1f2558cf1d"
  },
  "kernelspec": {
   "display_name": "Python 3.7.5 ('env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
