{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# https://www.youtube.com/watch?v=ovB0ddFtzzA&t=876s\n",
    "\n",
    "class patchembed(nn.Module):\n",
    "    \"\"\" 원본이미지 -> 패치이미지로 만듬 패치 이미지 임베드\n",
    "    \n",
    "    Paramters\n",
    "    ---------\n",
    "    img_size : int\n",
    "        이미지의 사이즈 (정사각형)\n",
    "        변수값 들어갈때는 (img_size,img_size)로 들어감\n",
    "    \n",
    "    patch_size : int\n",
    "        패치가 될 사이즈\n",
    "        변수값 들어갈때는 (patch_size,patch_size)로 들어감\n",
    "    \n",
    "    int_chans : int\n",
    "        입력이미지 채널수\n",
    "        \n",
    "    embed_dim : int\n",
    "        임베딩할 차원\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self,img_size,patch_size,int_chans=3,embed_dim=768) -> None:\n",
    "        super().__init__()\n",
    "        self.img_size = img_size\n",
    "        self.patch_size = patch_size\n",
    "        \n",
    "        ## 패치 갯수\n",
    "        self.n_patches = (img_size // patch_size)**2\n",
    "\n",
    "        self.proj = nn.Conv2d(int_chans,embed_dim,kernel_size=patch_size,stride=patch_size)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        \"\"\" 피드포워드 계산\n",
    "        \n",
    "        Parameters\n",
    "        -----------\n",
    "        x : torch.Tensor\n",
    "            모양 '(배치,채널수,이미지사이즈,이미지사이즈)'\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        torch.tensor\n",
    "            모양 '(배치,패치갯수,임베딩 차원)'\n",
    "            \n",
    "        \"\"\"\n",
    "        \n",
    "        x = self.proj(x)\n",
    "        x = x.flatten(2) # (배치,임배딩차원수,패치수)\n",
    "        x = x.transpose(1,2) # (배치,패치수,임배딩차원수)\n",
    "        return x\n",
    "    \n",
    "    \n",
    "class Attention(nn.Module):\n",
    "    \"\"\" 어텐션 메커니즘\n",
    "    Parameters\n",
    "    ----------\n",
    "    dim : int\n",
    "        인풋 차원\n",
    "        \n",
    "    n_heads : int\n",
    "        어텐션 메카니즘 헤더 갯수\n",
    "\n",
    "    qkv_bias : bool\n",
    "        쿼리,키,벨류 바이어스 변수 설정할건지\n",
    "        \n",
    "    attn_p : float\n",
    "        드롭아웃 확률 (쿼리,키,벨류)\n",
    "    \n",
    "    proj_p : float\n",
    "        드롭아웃 확률 (출력 텐서)    \n",
    "    \n",
    "    \n",
    "    Attributes\n",
    "    ----------\n",
    "    scale : float\n",
    "        노멀라이징 \n",
    "    qkv : nn.Linear\n",
    "        키,쿼리,벨류\n",
    "        \n",
    "    proj : nn.Linear\n",
    "        어텐션 값들 덴스레이어\n",
    "        \n",
    "    attn_drop, proj_drop : nn.Dropout\n",
    "        드롭아웃 레이어    \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self,dim,n_heads=12,qkv_bias=True,attn_p=0.,proj_p=0.) -> None:\n",
    "        super().__init__()\n",
    "        self.n_heads = n_heads\n",
    "        self.dim = dim\n",
    "        self.head_dim = dim // n_heads # 멀티헤드 어텐션 헤드는... 인코더의 전체차원에서 n_heads만큼 나누어줌\n",
    "        self.scale = self.head_dim ** -0.5 ## 어텐션 벡터 스케일링\n",
    "        \n",
    "        \n",
    "        self.qkv = nn.Linear(dim,dim*3,bias=qkv_bias)\n",
    "        self.attn_drop = nn.Dropout(attn_p)\n",
    "        self.proj = nn.Linear(dim,dim) ## 멀티헤더 어텐션은 입력,출력 차원의 갯수는 똑같음\n",
    "        self.proj_drop = nn.Dropout(proj_p)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        \"\"\" 전방향 연산 시작, (멀티헤더 어텐션은 입력,출력 차원의 갯수는 똑같음)\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        x : torch.Tensor\n",
    "            모양 '(배치,패치수+1,dim)'\n",
    "            패치수+1은 앞에 클래스 토큰\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        torch.Tensor\n",
    "            모양 '(배치,패치수+1,dim)'\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        ## 배치수, 패치수, x의 차원\n",
    "        ## 여기서 패치수는 임베딩된 벡터라 하나의 토큰으로 보아도 무방함\n",
    "        n_samples, n_tokens, dim = x.shape\n",
    "        \n",
    "        \n",
    "        ## 멀티헤더 셀프 어텐션은 입력과 출력의 차원이 같아야하는데 맞지 않다면 오류임 \n",
    "        if dim != self.dim:\n",
    "            raise ValueError\n",
    "        \n",
    "        \n",
    "        ## qkv를 한꺼번에 계산 -> 리쉐이프\n",
    "        qkv = self.qkv(x) # (배치,패치+1,3*dim)\n",
    "        \n",
    "\n",
    "        qkv = qkv.reshape(n_samples,n_tokens,3,self.n_heads,self.head_dim) # (배치,패치수+1,3,해더수,해더 차원)\n",
    "        qkv = qkv.permute(2,0,3,1,4) # (3,배치,해더수,패치수+1,해더 차원)\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        ## 쿼리,키,벨류 값 가져오기\n",
    "        q,k,v = qkv[0],qkv[1],qkv[2]\n",
    "        \n",
    "        ## 키값 ??? \n",
    "        k_t = k.transpose(-2,-1) # (배치,해더수,해더차원,패치수+1)\n",
    "        \n",
    "        ## 두행렬을 곱하고 스케일 조정\n",
    "        dp = (q@k_t) * self.scale # (배치,해더수,패치수+1,패치수+1)\n",
    "        \n",
    "        \n",
    "        ## 어텐션 맵 만듬 (소프트 맥스 & 드롭아웃)\n",
    "        attn = dp.softmax(dim=-1)\n",
    "        attn = self.attn_drop(attn)\n",
    "        \n",
    "        \n",
    "        weighted_avg = attn @ v # (배치,해더수,패치수+1,해더차원)\n",
    "        weighted_avg = weighted_avg.transpose(1,2) # (배치,패치수+1,해더수,해더차원)\n",
    "        weighted_avg = weighted_avg.flatten(2) # (배치,패치수+1,)\n",
    "        \n",
    "        x = self.proj(weighted_avg)\n",
    "        x = self.proj_drop(x)\n",
    "        \n",
    "        \n",
    "        return x\n",
    "        \n",
    "class MLP(nn.Module):\n",
    "    \"\"\" 멀티 레이어\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    in_features: int\n",
    "        입력데이터 사이즈\n",
    "        \n",
    "    hidden_feactures : int\n",
    "        히든 레이어 갯수\n",
    "    \n",
    "    out_feactures : int\n",
    "        출력 사이즈\n",
    "    \n",
    "    p : float\n",
    "        드롭아웃 확률\n",
    "        \n",
    "    \"\"\"\n",
    "    def __init__(self,in_features,hidden_feactures,out_feactures,p=0.):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(in_features,hidden_feactures)\n",
    "        self.act = nn.GELU()\n",
    "        self.fc2 = nn.Linear(hidden_feactures,out_feactures)\n",
    "        self.drop = nn.Dropout(p)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.act(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.drop(x)\n",
    "        \n",
    "        return x\n",
    "        \n",
    "        \n",
    "        \n",
    "class Block(nn.Module):\n",
    "    \"\"\" 트랜스 포머 블럭\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    dim : int\n",
    "        임베딩 차원\n",
    "    \n",
    "    n_heads : int\n",
    "        어텐션 해더 갯수\n",
    "        \n",
    "    mlp_ratio : float\n",
    "        'dim'에 대한 'MLP' 모듈의 숨겨진 차원 크기를 결정\n",
    "    \n",
    "    qkv_bias : bool\n",
    "        키,쿼리,블럭 바이어스 변수 설정\n",
    "        \n",
    "    p, attn_p : float\n",
    "        드롭아웃 확률\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self,dim,n_heads,mlp_ratio=4.0,qkv_bias=True,p=0,attn_p=0):\n",
    "        super().__init__()\n",
    "        self.norm1 = nn.LayerNorm(dim, eps=1e-6)\n",
    "        self.attn = Attention(dim,n_heads=n_heads,qkv_bias=qkv_bias,attn_p=attn_p,proj_p=p)\n",
    "        self.norm2 = nn.LayerNorm(dim,eps=1e-6)\n",
    "        \n",
    "        ## MLP레이어 임베딩차원은 -> 트랜스포머의 출력 벡터의 4배로   \n",
    "        hidden_feactures = int(dim*mlp_ratio)\n",
    "        self.mlp = MLP(\n",
    "            in_features=dim,\n",
    "            hidden_feactures=hidden_feactures,\n",
    "            out_feactures=dim,\n",
    "        )\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x = x + self.attn(self.norm1(x))\n",
    "        x = x + self.mlp(self.norm2(x))\n",
    "        \n",
    "        return x\n",
    "        \n",
    "          \n",
    "class Vit(nn.Module):\n",
    "    def __init__(self,\n",
    "                 img_size=256,\n",
    "                 patch_size=16,\n",
    "                 in_chans=3,\n",
    "                 n_classes=1000,\n",
    "                 embed_dim=768,\n",
    "                 depth=1,\n",
    "                 n_heads=12,\n",
    "                 mlp_ratio=4.,\n",
    "                 qkv_bias=False,\n",
    "                 p=0.,\n",
    "                 attn_p=0.,                 \n",
    "                 ):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.patch_embed = patchembed(\n",
    "            img_size=img_size,\n",
    "            patch_size=patch_size,\n",
    "            int_chans=in_chans,\n",
    "            embed_dim=embed_dim\n",
    "        )\n",
    "        \n",
    "        ## 임베드 벡터의 맨앞에 붙일 클래스 토큰\n",
    "        self.cls_token = nn.Parameter(torch.zeros(1,1,embed_dim))\n",
    "        \n",
    "        ## 포지션 파라미터들\n",
    "        self.pos_embed = nn.Parameter(torch.zeros(1,1+self.patch_embed.n_patches,embed_dim))\n",
    "        \n",
    "        self.pos_drop = nn.Dropout(p=p)\n",
    "        \n",
    "        self.blocks = nn.ModuleList(\n",
    "            [\n",
    "                Block(\n",
    "                    dim = embed_dim,\n",
    "                    n_heads=n_heads,\n",
    "                    mlp_ratio=mlp_ratio,\n",
    "                    qkv_bias=qkv_bias,\n",
    "                    p=p,\n",
    "                    attn_p=attn_p,                    \n",
    "                )\n",
    "                for _ in range(depth)\n",
    "            ]\n",
    "        )\n",
    "        \n",
    "        self.norm = nn.LayerNorm(embed_dim,eps=1e-6)\n",
    "        self.head = nn.Linear(embed_dim,n_classes)\n",
    "    \n",
    "    def forward(self,x):\n",
    "        ## 배치수\n",
    "        n_samples = x.shape[0]\n",
    "        x = self.patch_embed(x)\n",
    "        \n",
    "        cls_token = self.cls_token.expand(n_samples,-1,-1) # (배치,1,임베드차원)\n",
    "        \n",
    "        ## cls 토큰을 붙임\n",
    "        x = torch.cat((cls_token,x),dim=1)\n",
    "        \n",
    "        x = x + self.pos_embed # (qocl,1+패치수,임베딩차원)\n",
    "        x = self.pos_drop(x)\n",
    "        \n",
    "        for block in self.blocks:\n",
    "            x = block(x)\n",
    "        \n",
    "        x = self.norm(x)\n",
    "        \n",
    "        cls_token_final = x[:,0] # vit 마지막 결과값 가져옴\n",
    "        x = self.head(cls_token_final)\n",
    "        \n",
    "        return x  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "## 데이터 로더\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "img_size = 32\n",
    "batch_size = 512\n",
    "\n",
    "mean = (0.4914, 0.4822, 0.4465)\n",
    "std = (0.2023, 0.1994, 0.2010)\n",
    "train_transform = transforms.Compose([transforms.Resize(img_size), transforms.RandomCrop(img_size, padding=2),\n",
    "                                        transforms.RandomHorizontalFlip(), transforms.ToTensor(), transforms.Normalize(mean, std)])\n",
    "test_transform = transforms.Compose([transforms.Resize(img_size), transforms.ToTensor(),\n",
    "                                        transforms.Normalize(mean, std)])\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=train_transform)\n",
    "valset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=test_transform)\n",
    "\n",
    "\n",
    "trainloader = DataLoader(trainset, batch_size=batch_size)\n",
    "valloader = DataLoader(valset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Vit(\n",
       "  (patch_embed): patchembed(\n",
       "    (proj): Conv2d(3, 128, kernel_size=(4, 4), stride=(4, 4))\n",
       "  )\n",
       "  (pos_drop): Dropout(p=0.0, inplace=False)\n",
       "  (blocks): ModuleList(\n",
       "    (0): Block(\n",
       "      (norm1): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=128, out_features=384, bias=False)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (norm2): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): MLP(\n",
       "        (fc1): Linear(in_features=128, out_features=512, bias=True)\n",
       "        (act): GELU()\n",
       "        (fc2): Linear(in_features=512, out_features=128, bias=True)\n",
       "        (drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (1): Block(\n",
       "      (norm1): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=128, out_features=384, bias=False)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (norm2): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): MLP(\n",
       "        (fc1): Linear(in_features=128, out_features=512, bias=True)\n",
       "        (act): GELU()\n",
       "        (fc2): Linear(in_features=512, out_features=128, bias=True)\n",
       "        (drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (2): Block(\n",
       "      (norm1): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=128, out_features=384, bias=False)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (norm2): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): MLP(\n",
       "        (fc1): Linear(in_features=128, out_features=512, bias=True)\n",
       "        (act): GELU()\n",
       "        (fc2): Linear(in_features=512, out_features=128, bias=True)\n",
       "        (drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (3): Block(\n",
       "      (norm1): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=128, out_features=384, bias=False)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (norm2): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): MLP(\n",
       "        (fc1): Linear(in_features=128, out_features=512, bias=True)\n",
       "        (act): GELU()\n",
       "        (fc2): Linear(in_features=512, out_features=128, bias=True)\n",
       "        (drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (4): Block(\n",
       "      (norm1): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=128, out_features=384, bias=False)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (norm2): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): MLP(\n",
       "        (fc1): Linear(in_features=128, out_features=512, bias=True)\n",
       "        (act): GELU()\n",
       "        (fc2): Linear(in_features=512, out_features=128, bias=True)\n",
       "        (drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (5): Block(\n",
       "      (norm1): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=128, out_features=384, bias=False)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (norm2): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): MLP(\n",
       "        (fc1): Linear(in_features=128, out_features=512, bias=True)\n",
       "        (act): GELU()\n",
       "        (fc2): Linear(in_features=512, out_features=128, bias=True)\n",
       "        (drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (6): Block(\n",
       "      (norm1): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=128, out_features=384, bias=False)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (norm2): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): MLP(\n",
       "        (fc1): Linear(in_features=128, out_features=512, bias=True)\n",
       "        (act): GELU()\n",
       "        (fc2): Linear(in_features=512, out_features=128, bias=True)\n",
       "        (drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (7): Block(\n",
       "      (norm1): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=128, out_features=384, bias=False)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (norm2): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): MLP(\n",
       "        (fc1): Linear(in_features=128, out_features=512, bias=True)\n",
       "        (act): GELU()\n",
       "        (fc2): Linear(in_features=512, out_features=128, bias=True)\n",
       "        (drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (8): Block(\n",
       "      (norm1): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=128, out_features=384, bias=False)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (norm2): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): MLP(\n",
       "        (fc1): Linear(in_features=128, out_features=512, bias=True)\n",
       "        (act): GELU()\n",
       "        (fc2): Linear(in_features=512, out_features=128, bias=True)\n",
       "        (drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (9): Block(\n",
       "      (norm1): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=128, out_features=384, bias=False)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (norm2): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): MLP(\n",
       "        (fc1): Linear(in_features=128, out_features=512, bias=True)\n",
       "        (act): GELU()\n",
       "        (fc2): Linear(in_features=512, out_features=128, bias=True)\n",
       "        (drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (10): Block(\n",
       "      (norm1): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=128, out_features=384, bias=False)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (norm2): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): MLP(\n",
       "        (fc1): Linear(in_features=128, out_features=512, bias=True)\n",
       "        (act): GELU()\n",
       "        (fc2): Linear(in_features=512, out_features=128, bias=True)\n",
       "        (drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (11): Block(\n",
       "      (norm1): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=128, out_features=384, bias=False)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (norm2): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): MLP(\n",
       "        (fc1): Linear(in_features=128, out_features=512, bias=True)\n",
       "        (act): GELU()\n",
       "        (fc2): Linear(in_features=512, out_features=128, bias=True)\n",
       "        (drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (norm): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "  (head): Linear(in_features=128, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 모델 선언\n",
    "device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = Vit(img_size=img_size,patch_size=4,in_chans=3,n_classes=10,embed_dim=128,n_heads=8,depth=12)\n",
    "\n",
    "# model = Vit(img_size=img_size,patch_size=4,in_chans=3,n_classes=10)\n",
    "\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0\t98/0]\t loss : 2.4699 \t accuracy : 7.42% \t38/512\n",
      "[0\t98/40]\t loss : 1.8342 \t accuracy : 26.76% \t137/512\n",
      "[0\t98/80]\t loss : 1.6349 \t accuracy : 39.26% \t201/512\n",
      "0 epoch 평균\tloss : 1.8583297814641682 \t accuracy : 29.97% \t 15037/50176\n",
      "\n",
      "\n",
      "[1\t98/0]\t loss : 1.7302 \t accuracy : 36.72% \t188/512\n",
      "[1\t98/40]\t loss : 1.5209 \t accuracy : 43.75% \t224/512\n",
      "[1\t98/80]\t loss : 1.4569 \t accuracy : 44.34% \t227/512\n",
      "1 epoch 평균\tloss : 1.5373048137645327 \t accuracy : 42.94% \t 21546/50176\n",
      "\n",
      "\n",
      "[2\t98/0]\t loss : 1.4012 \t accuracy : 49.02% \t251/512\n",
      "[2\t98/40]\t loss : 1.3398 \t accuracy : 52.93% \t271/512\n",
      "[2\t98/80]\t loss : 1.2834 \t accuracy : 52.54% \t269/512\n",
      "2 epoch 평균\tloss : 1.335458276223163 \t accuracy : 51.03% \t 25605/50176\n",
      "\n",
      "\n",
      "[3\t98/0]\t loss : 1.1753 \t accuracy : 56.45% \t289/512\n",
      "[3\t98/40]\t loss : 1.2518 \t accuracy : 55.08% \t282/512\n",
      "[3\t98/80]\t loss : 1.2107 \t accuracy : 55.08% \t282/512\n",
      "3 epoch 평균\tloss : 1.2233284480717714 \t accuracy : 55.30% \t 27749/50176\n",
      "\n",
      "\n",
      "[4\t98/0]\t loss : 1.1388 \t accuracy : 58.98% \t302/512\n",
      "[4\t98/40]\t loss : 1.1291 \t accuracy : 60.16% \t308/512\n",
      "[4\t98/80]\t loss : 1.1181 \t accuracy : 58.01% \t297/512\n",
      "4 epoch 평균\tloss : 1.1417003079336518 \t accuracy : 58.60% \t 29405/50176\n",
      "\n",
      "\n",
      "[5\t98/0]\t loss : 0.9857 \t accuracy : 62.70% \t321/512\n",
      "[5\t98/40]\t loss : 1.1632 \t accuracy : 60.35% \t309/512\n",
      "[5\t98/80]\t loss : 1.0827 \t accuracy : 62.30% \t319/512\n",
      "5 epoch 평균\tloss : 1.0800081594866149 \t accuracy : 60.82% \t 30517/50176\n",
      "\n",
      "\n",
      "[6\t98/0]\t loss : 0.9622 \t accuracy : 66.99% \t343/512\n",
      "[6\t98/40]\t loss : 1.0654 \t accuracy : 63.48% \t325/512\n",
      "[6\t98/80]\t loss : 1.0155 \t accuracy : 62.50% \t320/512\n",
      "6 epoch 평균\tloss : 1.0160991300125513 \t accuracy : 63.23% \t 31728/50176\n",
      "\n",
      "\n",
      "[7\t98/0]\t loss : 0.9655 \t accuracy : 65.62% \t336/512\n",
      "[7\t98/40]\t loss : 0.9549 \t accuracy : 66.60% \t341/512\n",
      "[7\t98/80]\t loss : 0.9840 \t accuracy : 64.65% \t331/512\n",
      "7 epoch 평균\tloss : 0.9730797300533373 \t accuracy : 65.23% \t 32732/50176\n",
      "\n",
      "\n",
      "[8\t98/0]\t loss : 0.8405 \t accuracy : 69.34% \t355/512\n",
      "[8\t98/40]\t loss : 0.8859 \t accuracy : 70.31% \t360/512\n",
      "[8\t98/80]\t loss : 0.9028 \t accuracy : 67.38% \t345/512\n",
      "8 epoch 평균\tloss : 0.9221482538447089 \t accuracy : 67.00% \t 33616/50176\n",
      "\n",
      "\n",
      "[9\t98/0]\t loss : 0.8439 \t accuracy : 71.29% \t365/512\n",
      "[9\t98/40]\t loss : 0.8561 \t accuracy : 68.55% \t351/512\n",
      "[9\t98/80]\t loss : 0.8881 \t accuracy : 68.55% \t351/512\n",
      "9 epoch 평균\tloss : 0.8815720257710434 \t accuracy : 68.47% \t 34358/50176\n",
      "\n",
      "\n",
      "[10\t98/0]\t loss : 0.7718 \t accuracy : 74.02% \t379/512\n",
      "[10\t98/40]\t loss : 0.8180 \t accuracy : 71.09% \t364/512\n",
      "[10\t98/80]\t loss : 0.8797 \t accuracy : 69.34% \t355/512\n",
      "10 epoch 평균\tloss : 0.8498934330988906 \t accuracy : 69.44% \t 34841/50176\n",
      "\n",
      "\n",
      "[11\t98/0]\t loss : 0.7419 \t accuracy : 74.02% \t379/512\n",
      "[11\t98/40]\t loss : 0.8003 \t accuracy : 70.31% \t360/512\n",
      "[11\t98/80]\t loss : 0.7905 \t accuracy : 71.09% \t364/512\n",
      "11 epoch 평균\tloss : 0.8102815972298993 \t accuracy : 70.93% \t 35590/50176\n",
      "\n",
      "\n",
      "[12\t98/0]\t loss : 0.7338 \t accuracy : 75.59% \t387/512\n",
      "[12\t98/40]\t loss : 0.7629 \t accuracy : 72.85% \t373/512\n",
      "[12\t98/80]\t loss : 0.7850 \t accuracy : 71.29% \t365/512\n",
      "12 epoch 평균\tloss : 0.7878277320034651 \t accuracy : 72.03% \t 36142/50176\n",
      "\n",
      "\n",
      "[13\t98/0]\t loss : 0.7212 \t accuracy : 75.39% \t386/512\n",
      "[13\t98/40]\t loss : 0.7226 \t accuracy : 75.20% \t385/512\n",
      "[13\t98/80]\t loss : 0.7489 \t accuracy : 73.63% \t377/512\n",
      "13 epoch 평균\tloss : 0.7460264076991961 \t accuracy : 73.30% \t 36777/50176\n",
      "\n",
      "\n",
      "[14\t98/0]\t loss : 0.6890 \t accuracy : 75.20% \t385/512\n",
      "[14\t98/40]\t loss : 0.6821 \t accuracy : 77.54% \t397/512\n",
      "[14\t98/80]\t loss : 0.7550 \t accuracy : 72.85% \t373/512\n",
      "14 epoch 평균\tloss : 0.7237890502628014 \t accuracy : 74.39% \t 37326/50176\n",
      "\n",
      "\n",
      "[15\t98/0]\t loss : 0.6045 \t accuracy : 80.08% \t410/512\n",
      "[15\t98/40]\t loss : 0.7045 \t accuracy : 74.02% \t379/512\n",
      "[15\t98/80]\t loss : 0.6972 \t accuracy : 76.95% \t394/512\n",
      "15 epoch 평균\tloss : 0.6943529102267051 \t accuracy : 75.21% \t 37735/50176\n",
      "\n",
      "\n",
      "[16\t98/0]\t loss : 0.6267 \t accuracy : 79.49% \t407/512\n",
      "[16\t98/40]\t loss : 0.5947 \t accuracy : 78.32% \t401/512\n",
      "[16\t98/80]\t loss : 0.6254 \t accuracy : 77.54% \t397/512\n",
      "16 epoch 평균\tloss : 0.6713222812633127 \t accuracy : 75.88% \t 38076/50176\n",
      "\n",
      "\n",
      "[17\t98/0]\t loss : 0.6212 \t accuracy : 78.32% \t401/512\n",
      "[17\t98/40]\t loss : 0.6022 \t accuracy : 80.47% \t412/512\n",
      "[17\t98/80]\t loss : 0.6757 \t accuracy : 75.78% \t388/512\n",
      "17 epoch 평균\tloss : 0.6534154609758027 \t accuracy : 76.76% \t 38514/50176\n",
      "\n",
      "\n",
      "[18\t98/0]\t loss : 0.5563 \t accuracy : 79.69% \t408/512\n",
      "[18\t98/40]\t loss : 0.6156 \t accuracy : 76.95% \t394/512\n",
      "[18\t98/80]\t loss : 0.5938 \t accuracy : 78.91% \t404/512\n",
      "18 epoch 평균\tloss : 0.6333325304547137 \t accuracy : 77.34% \t 38805/50176\n",
      "\n",
      "\n",
      "[19\t98/0]\t loss : 0.5771 \t accuracy : 80.27% \t411/512\n",
      "[19\t98/40]\t loss : 0.5672 \t accuracy : 80.08% \t410/512\n",
      "[19\t98/80]\t loss : 0.5330 \t accuracy : 81.84% \t419/512\n",
      "19 epoch 평균\tloss : 0.603299828816433 \t accuracy : 78.15% \t 39215/50176\n",
      "\n",
      "\n",
      "[20\t98/0]\t loss : 0.4881 \t accuracy : 84.38% \t432/512\n",
      "[20\t98/40]\t loss : 0.5508 \t accuracy : 82.62% \t423/512\n",
      "[20\t98/80]\t loss : 0.5336 \t accuracy : 82.03% \t420/512\n",
      "20 epoch 평균\tloss : 0.5810476018458 \t accuracy : 79.18% \t 39728/50176\n",
      "\n",
      "\n",
      "[21\t98/0]\t loss : 0.4800 \t accuracy : 82.62% \t423/512\n",
      "[21\t98/40]\t loss : 0.5350 \t accuracy : 81.84% \t419/512\n",
      "[21\t98/80]\t loss : 0.5515 \t accuracy : 80.86% \t414/512\n",
      "21 epoch 평균\tloss : 0.5645182144885161 \t accuracy : 79.70% \t 39990/50176\n",
      "\n",
      "\n",
      "[22\t98/0]\t loss : 0.4809 \t accuracy : 83.98% \t430/512\n",
      "[22\t98/40]\t loss : 0.5947 \t accuracy : 78.71% \t403/512\n",
      "[22\t98/80]\t loss : 0.5567 \t accuracy : 79.69% \t408/512\n",
      "22 epoch 평균\tloss : 0.5517065290893827 \t accuracy : 80.28% \t 40280/50176\n",
      "\n",
      "\n",
      "[23\t98/0]\t loss : 0.4465 \t accuracy : 84.38% \t432/512\n",
      "[23\t98/40]\t loss : 0.4668 \t accuracy : 82.23% \t421/512\n",
      "[23\t98/80]\t loss : 0.5001 \t accuracy : 82.62% \t423/512\n",
      "23 epoch 평균\tloss : 0.5210533510057295 \t accuracy : 81.28% \t 40783/50176\n",
      "\n",
      "\n",
      "[24\t98/0]\t loss : 0.4875 \t accuracy : 81.84% \t419/512\n",
      "[24\t98/40]\t loss : 0.4751 \t accuracy : 80.27% \t411/512\n",
      "[24\t98/80]\t loss : 0.4601 \t accuracy : 83.59% \t428/512\n",
      "24 epoch 평균\tloss : 0.5054169053934062 \t accuracy : 81.62% \t 40955/50176\n",
      "\n",
      "\n",
      "[25\t98/0]\t loss : 0.4265 \t accuracy : 86.13% \t441/512\n",
      "[25\t98/40]\t loss : 0.4310 \t accuracy : 84.38% \t432/512\n",
      "[25\t98/80]\t loss : 0.5024 \t accuracy : 79.69% \t408/512\n",
      "25 epoch 평균\tloss : 0.4813725726336847 \t accuracy : 82.56% \t 41427/50176\n",
      "\n",
      "\n",
      "[26\t98/0]\t loss : 0.4046 \t accuracy : 86.13% \t441/512\n",
      "[26\t98/40]\t loss : 0.4581 \t accuracy : 83.98% \t430/512\n",
      "[26\t98/80]\t loss : 0.4349 \t accuracy : 84.96% \t435/512\n",
      "26 epoch 평균\tloss : 0.47229291012092506 \t accuracy : 82.85% \t 41571/50176\n",
      "\n",
      "\n",
      "[27\t98/0]\t loss : 0.3360 \t accuracy : 87.70% \t449/512\n",
      "[27\t98/40]\t loss : 0.4046 \t accuracy : 86.13% \t441/512\n",
      "[27\t98/80]\t loss : 0.4833 \t accuracy : 82.23% \t421/512\n",
      "27 epoch 평균\tloss : 0.44349708544964694 \t accuracy : 83.94% \t 42118/50176\n",
      "\n",
      "\n",
      "[28\t98/0]\t loss : 0.4024 \t accuracy : 87.11% \t446/512\n",
      "[28\t98/40]\t loss : 0.4033 \t accuracy : 84.18% \t431/512\n",
      "[28\t98/80]\t loss : 0.4701 \t accuracy : 81.84% \t419/512\n",
      "28 epoch 평균\tloss : 0.43166462377626075 \t accuracy : 84.36% \t 42330/50176\n",
      "\n",
      "\n",
      "[29\t98/0]\t loss : 0.3696 \t accuracy : 86.33% \t442/512\n",
      "[29\t98/40]\t loss : 0.4071 \t accuracy : 83.98% \t430/512\n",
      "[29\t98/80]\t loss : 0.3973 \t accuracy : 85.94% \t440/512\n",
      "29 epoch 평균\tloss : 0.41432954979186165 \t accuracy : 85.04% \t 42670/50176\n",
      "\n",
      "\n",
      "[30\t98/0]\t loss : 0.3662 \t accuracy : 87.50% \t448/512\n",
      "[30\t98/40]\t loss : 0.3655 \t accuracy : 85.74% \t439/512\n",
      "[30\t98/80]\t loss : 0.4074 \t accuracy : 84.96% \t435/512\n",
      "30 epoch 평균\tloss : 0.3963661102616059 \t accuracy : 85.35% \t 42824/50176\n",
      "\n",
      "\n",
      "[31\t98/0]\t loss : 0.3177 \t accuracy : 88.87% \t455/512\n",
      "[31\t98/40]\t loss : 0.4018 \t accuracy : 86.13% \t441/512\n",
      "[31\t98/80]\t loss : 0.3919 \t accuracy : 86.72% \t444/512\n",
      "31 epoch 평균\tloss : 0.3829695409049793 \t accuracy : 85.92% \t 43110/50176\n",
      "\n",
      "\n",
      "[32\t98/0]\t loss : 0.3567 \t accuracy : 86.13% \t441/512\n",
      "[32\t98/40]\t loss : 0.3098 \t accuracy : 88.87% \t455/512\n",
      "[32\t98/80]\t loss : 0.3236 \t accuracy : 87.70% \t449/512\n",
      "32 epoch 평균\tloss : 0.3605510592460633 \t accuracy : 86.85% \t 43576/50176\n",
      "\n",
      "\n",
      "[33\t98/0]\t loss : 0.2716 \t accuracy : 89.06% \t456/512\n",
      "[33\t98/40]\t loss : 0.3216 \t accuracy : 88.09% \t451/512\n",
      "[33\t98/80]\t loss : 0.3405 \t accuracy : 87.50% \t448/512\n",
      "33 epoch 평균\tloss : 0.34321657461779476 \t accuracy : 87.44% \t 43875/50176\n",
      "\n",
      "\n",
      "[34\t98/0]\t loss : 0.3005 \t accuracy : 90.62% \t464/512\n",
      "[34\t98/40]\t loss : 0.2610 \t accuracy : 89.65% \t459/512\n",
      "[34\t98/80]\t loss : 0.3703 \t accuracy : 87.70% \t449/512\n",
      "34 epoch 평균\tloss : 0.32684999902029427 \t accuracy : 87.99% \t 44152/50176\n",
      "\n",
      "\n",
      "[35\t98/0]\t loss : 0.2648 \t accuracy : 90.82% \t465/512\n",
      "[35\t98/40]\t loss : 0.2937 \t accuracy : 88.87% \t455/512\n",
      "[35\t98/80]\t loss : 0.3096 \t accuracy : 89.26% \t457/512\n",
      "35 epoch 평균\tloss : 0.3195163940592687 \t accuracy : 88.39% \t 44350/50176\n",
      "\n",
      "\n",
      "[36\t98/0]\t loss : 0.2621 \t accuracy : 90.43% \t463/512\n",
      "[36\t98/40]\t loss : 0.2799 \t accuracy : 90.62% \t464/512\n",
      "[36\t98/80]\t loss : 0.2843 \t accuracy : 89.65% \t459/512\n",
      "36 epoch 평균\tloss : 0.3067095086586718 \t accuracy : 88.70% \t 44504/50176\n",
      "\n",
      "\n",
      "[37\t98/0]\t loss : 0.1973 \t accuracy : 93.16% \t477/512\n",
      "[37\t98/40]\t loss : 0.2630 \t accuracy : 91.02% \t466/512\n",
      "[37\t98/80]\t loss : 0.2888 \t accuracy : 90.04% \t461/512\n",
      "37 epoch 평균\tloss : 0.29257723111279166 \t accuracy : 89.18% \t 44748/50176\n",
      "\n",
      "\n",
      "[38\t98/0]\t loss : 0.2243 \t accuracy : 91.60% \t469/512\n",
      "[38\t98/40]\t loss : 0.2606 \t accuracy : 89.84% \t460/512\n",
      "[38\t98/80]\t loss : 0.2661 \t accuracy : 90.82% \t465/512\n",
      "38 epoch 평균\tloss : 0.2802130680303185 \t accuracy : 89.61% \t 44961/50176\n",
      "\n",
      "\n",
      "[39\t98/0]\t loss : 0.2296 \t accuracy : 91.21% \t467/512\n",
      "[39\t98/40]\t loss : 0.2136 \t accuracy : 91.60% \t469/512\n",
      "[39\t98/80]\t loss : 0.2565 \t accuracy : 90.23% \t462/512\n",
      "39 epoch 평균\tloss : 0.25526892165748427 \t accuracy : 90.39% \t 45355/50176\n",
      "\n",
      "\n",
      "[40\t98/0]\t loss : 0.2100 \t accuracy : 92.38% \t473/512\n",
      "[40\t98/40]\t loss : 0.2423 \t accuracy : 90.23% \t462/512\n",
      "[40\t98/80]\t loss : 0.2435 \t accuracy : 90.43% \t463/512\n",
      "40 epoch 평균\tloss : 0.25691499135323936 \t accuracy : 90.41% \t 45364/50176\n",
      "\n",
      "\n",
      "[41\t98/0]\t loss : 0.2190 \t accuracy : 91.99% \t471/512\n",
      "[41\t98/40]\t loss : 0.1825 \t accuracy : 93.36% \t478/512\n",
      "[41\t98/80]\t loss : 0.2933 \t accuracy : 88.87% \t455/512\n",
      "41 epoch 평균\tloss : 0.24556842233453488 \t accuracy : 90.82% \t 45570/50176\n",
      "\n",
      "\n",
      "[42\t98/0]\t loss : 0.1943 \t accuracy : 92.38% \t473/512\n",
      "[42\t98/40]\t loss : 0.1828 \t accuracy : 93.75% \t480/512\n",
      "[42\t98/80]\t loss : 0.2442 \t accuracy : 92.19% \t472/512\n",
      "42 epoch 평균\tloss : 0.24333777354688052 \t accuracy : 90.91% \t 45615/50176\n",
      "\n",
      "\n",
      "[43\t98/0]\t loss : 0.2321 \t accuracy : 91.80% \t470/512\n",
      "[43\t98/40]\t loss : 0.2135 \t accuracy : 93.36% \t478/512\n",
      "[43\t98/80]\t loss : 0.2139 \t accuracy : 91.21% \t467/512\n",
      "43 epoch 평균\tloss : 0.22232382440445367 \t accuracy : 91.79% \t 46055/50176\n",
      "\n",
      "\n",
      "[44\t98/0]\t loss : 0.2363 \t accuracy : 91.99% \t471/512\n",
      "[44\t98/40]\t loss : 0.1603 \t accuracy : 94.34% \t483/512\n",
      "[44\t98/80]\t loss : 0.2022 \t accuracy : 92.19% \t472/512\n",
      "44 epoch 평균\tloss : 0.21207415578620772 \t accuracy : 92.10% \t 46211/50176\n",
      "\n",
      "\n",
      "[45\t98/0]\t loss : 0.1699 \t accuracy : 93.55% \t479/512\n",
      "[45\t98/40]\t loss : 0.1312 \t accuracy : 96.09% \t492/512\n",
      "[45\t98/80]\t loss : 0.2306 \t accuracy : 92.19% \t472/512\n",
      "45 epoch 평균\tloss : 0.19216896145015347 \t accuracy : 92.79% \t 46557/50176\n",
      "\n",
      "\n",
      "[46\t98/0]\t loss : 0.1928 \t accuracy : 92.38% \t473/512\n",
      "[46\t98/40]\t loss : 0.1409 \t accuracy : 95.12% \t487/512\n",
      "[46\t98/80]\t loss : 0.2056 \t accuracy : 93.16% \t477/512\n",
      "46 epoch 평균\tloss : 0.19239029394728793 \t accuracy : 92.85% \t 46590/50176\n",
      "\n",
      "\n",
      "[47\t98/0]\t loss : 0.2083 \t accuracy : 91.60% \t469/512\n",
      "[47\t98/40]\t loss : 0.1692 \t accuracy : 93.16% \t477/512\n",
      "[47\t98/80]\t loss : 0.1728 \t accuracy : 92.97% \t476/512\n",
      "47 epoch 평균\tloss : 0.1834856919488128 \t accuracy : 93.05% \t 46688/50176\n",
      "\n",
      "\n",
      "[48\t98/0]\t loss : 0.1620 \t accuracy : 93.75% \t480/512\n",
      "[48\t98/40]\t loss : 0.1414 \t accuracy : 94.34% \t483/512\n",
      "[48\t98/80]\t loss : 0.1485 \t accuracy : 95.12% \t487/512\n",
      "48 epoch 평균\tloss : 0.17379388730136713 \t accuracy : 93.46% \t 46894/50176\n",
      "\n",
      "\n",
      "[49\t98/0]\t loss : 0.1600 \t accuracy : 93.36% \t478/512\n",
      "[49\t98/40]\t loss : 0.1679 \t accuracy : 93.95% \t481/512\n",
      "[49\t98/80]\t loss : 0.1848 \t accuracy : 93.36% \t478/512\n",
      "49 epoch 평균\tloss : 0.17236988634175182 \t accuracy : 93.45% \t 46889/50176\n",
      "\n",
      "\n",
      "[50\t98/0]\t loss : 0.1477 \t accuracy : 94.92% \t486/512\n",
      "[50\t98/40]\t loss : 0.1392 \t accuracy : 95.51% \t489/512\n",
      "[50\t98/80]\t loss : 0.1534 \t accuracy : 95.31% \t488/512\n",
      "50 epoch 평균\tloss : 0.15495236955431047 \t accuracy : 94.14% \t 47238/50176\n",
      "\n",
      "\n",
      "[51\t98/0]\t loss : 0.1179 \t accuracy : 96.48% \t494/512\n",
      "[51\t98/40]\t loss : 0.1702 \t accuracy : 93.95% \t481/512\n",
      "[51\t98/80]\t loss : 0.1438 \t accuracy : 93.75% \t480/512\n",
      "51 epoch 평균\tloss : 0.14699475718091945 \t accuracy : 94.59% \t 47460/50176\n",
      "\n",
      "\n",
      "[52\t98/0]\t loss : 0.1405 \t accuracy : 94.92% \t486/512\n",
      "[52\t98/40]\t loss : 0.1131 \t accuracy : 95.51% \t489/512\n",
      "[52\t98/80]\t loss : 0.1221 \t accuracy : 95.90% \t491/512\n",
      "52 epoch 평균\tloss : 0.13904035273863344 \t accuracy : 94.72% \t 47525/50176\n",
      "\n",
      "\n",
      "[53\t98/0]\t loss : 0.1224 \t accuracy : 94.92% \t486/512\n",
      "[53\t98/40]\t loss : 0.1347 \t accuracy : 95.51% \t489/512\n",
      "[53\t98/80]\t loss : 0.1204 \t accuracy : 96.29% \t493/512\n",
      "53 epoch 평균\tloss : 0.1350529202241071 \t accuracy : 94.81% \t 47574/50176\n",
      "\n",
      "\n",
      "[54\t98/0]\t loss : 0.0962 \t accuracy : 96.48% \t494/512\n",
      "[54\t98/40]\t loss : 0.1550 \t accuracy : 94.92% \t486/512\n",
      "[54\t98/80]\t loss : 0.1123 \t accuracy : 95.70% \t490/512\n",
      "54 epoch 평균\tloss : 0.12719521473865109 \t accuracy : 95.09% \t 47712/50176\n",
      "\n",
      "\n",
      "[55\t98/0]\t loss : 0.1159 \t accuracy : 95.90% \t491/512\n",
      "[55\t98/40]\t loss : 0.1611 \t accuracy : 94.14% \t482/512\n",
      "[55\t98/80]\t loss : 0.1205 \t accuracy : 95.12% \t487/512\n",
      "55 epoch 평균\tloss : 0.12308033692593474 \t accuracy : 95.34% \t 47839/50176\n",
      "\n",
      "\n",
      "[56\t98/0]\t loss : 0.1253 \t accuracy : 95.12% \t487/512\n",
      "[56\t98/40]\t loss : 0.1291 \t accuracy : 95.70% \t490/512\n",
      "[56\t98/80]\t loss : 0.1046 \t accuracy : 95.90% \t491/512\n",
      "56 epoch 평균\tloss : 0.11757670464564336 \t accuracy : 95.48% \t 47907/50176\n",
      "\n",
      "\n",
      "[57\t98/0]\t loss : 0.1035 \t accuracy : 96.68% \t495/512\n",
      "[57\t98/40]\t loss : 0.1055 \t accuracy : 96.88% \t496/512\n",
      "[57\t98/80]\t loss : 0.0955 \t accuracy : 96.68% \t495/512\n",
      "57 epoch 평균\tloss : 0.11165190685768518 \t accuracy : 95.76% \t 48049/50176\n",
      "\n",
      "\n",
      "[58\t98/0]\t loss : 0.0731 \t accuracy : 97.27% \t498/512\n",
      "[58\t98/40]\t loss : 0.1035 \t accuracy : 94.92% \t486/512\n",
      "[58\t98/80]\t loss : 0.0938 \t accuracy : 97.27% \t498/512\n",
      "58 epoch 평균\tloss : 0.10653953679970334 \t accuracy : 95.88% \t 48109/50176\n",
      "\n",
      "\n",
      "[59\t98/0]\t loss : 0.1047 \t accuracy : 95.51% \t489/512\n",
      "[59\t98/40]\t loss : 0.1324 \t accuracy : 95.70% \t490/512\n",
      "[59\t98/80]\t loss : 0.0721 \t accuracy : 96.88% \t496/512\n",
      "59 epoch 평균\tloss : 0.1048330261695142 \t accuracy : 96.02% \t 48181/50176\n",
      "\n",
      "\n",
      "[60\t98/0]\t loss : 0.1454 \t accuracy : 94.92% \t486/512\n",
      "[60\t98/40]\t loss : 0.1189 \t accuracy : 96.88% \t496/512\n",
      "[60\t98/80]\t loss : 0.0899 \t accuracy : 97.46% \t499/512\n",
      "60 epoch 평균\tloss : 0.10192318606589516 \t accuracy : 96.04% \t 48190/50176\n",
      "\n",
      "\n",
      "[61\t98/0]\t loss : 0.0954 \t accuracy : 96.09% \t492/512\n",
      "[61\t98/40]\t loss : 0.0909 \t accuracy : 97.07% \t497/512\n",
      "[61\t98/80]\t loss : 0.0996 \t accuracy : 95.90% \t491/512\n",
      "61 epoch 평균\tloss : 0.09644626099996424 \t accuracy : 96.23% \t 48284/50176\n",
      "\n",
      "\n",
      "[62\t98/0]\t loss : 0.0914 \t accuracy : 97.27% \t498/512\n",
      "[62\t98/40]\t loss : 0.0780 \t accuracy : 97.46% \t499/512\n",
      "[62\t98/80]\t loss : 0.1142 \t accuracy : 95.70% \t490/512\n",
      "62 epoch 평균\tloss : 0.09784277228220385 \t accuracy : 96.26% \t 48299/50176\n",
      "\n",
      "\n",
      "[63\t98/0]\t loss : 0.0985 \t accuracy : 95.70% \t490/512\n",
      "[63\t98/40]\t loss : 0.0704 \t accuracy : 97.85% \t501/512\n",
      "[63\t98/80]\t loss : 0.0852 \t accuracy : 97.07% \t497/512\n",
      "63 epoch 평균\tloss : 0.09496773870623848 \t accuracy : 96.31% \t 48322/50176\n",
      "\n",
      "\n",
      "[64\t98/0]\t loss : 0.0882 \t accuracy : 96.68% \t495/512\n",
      "[64\t98/40]\t loss : 0.0831 \t accuracy : 96.68% \t495/512\n",
      "[64\t98/80]\t loss : 0.0863 \t accuracy : 97.27% \t498/512\n",
      "64 epoch 평균\tloss : 0.0917657881747095 \t accuracy : 96.48% \t 48409/50176\n",
      "\n",
      "\n",
      "[65\t98/0]\t loss : 0.0900 \t accuracy : 95.70% \t490/512\n",
      "[65\t98/40]\t loss : 0.1113 \t accuracy : 96.09% \t492/512\n",
      "[65\t98/80]\t loss : 0.0897 \t accuracy : 97.27% \t498/512\n",
      "65 epoch 평균\tloss : 0.0892012835841398 \t accuracy : 96.53% \t 48435/50176\n",
      "\n",
      "\n",
      "[66\t98/0]\t loss : 0.1094 \t accuracy : 95.31% \t488/512\n",
      "[66\t98/40]\t loss : 0.0799 \t accuracy : 97.07% \t497/512\n",
      "[66\t98/80]\t loss : 0.0829 \t accuracy : 97.66% \t500/512\n",
      "66 epoch 평균\tloss : 0.09003166192952469 \t accuracy : 96.45% \t 48394/50176\n",
      "\n",
      "\n",
      "[67\t98/0]\t loss : 0.0658 \t accuracy : 97.46% \t499/512\n",
      "[67\t98/40]\t loss : 0.0734 \t accuracy : 96.48% \t494/512\n",
      "[67\t98/80]\t loss : 0.0951 \t accuracy : 96.88% \t496/512\n",
      "67 epoch 평균\tloss : 0.08699086985113672 \t accuracy : 96.64% \t 48490/50176\n",
      "\n",
      "\n",
      "[68\t98/0]\t loss : 0.0747 \t accuracy : 97.66% \t500/512\n",
      "[68\t98/40]\t loss : 0.0995 \t accuracy : 95.31% \t488/512\n",
      "[68\t98/80]\t loss : 0.0544 \t accuracy : 97.85% \t501/512\n",
      "68 epoch 평균\tloss : 0.0842246791081769 \t accuracy : 96.66% \t 48501/50176\n",
      "\n",
      "\n",
      "[69\t98/0]\t loss : 0.0872 \t accuracy : 96.88% \t496/512\n",
      "[69\t98/40]\t loss : 0.0620 \t accuracy : 98.63% \t505/512\n",
      "[69\t98/80]\t loss : 0.0732 \t accuracy : 97.46% \t499/512\n",
      "69 epoch 평균\tloss : 0.08191527300799381 \t accuracy : 96.84% \t 48591/50176\n",
      "\n",
      "\n",
      "[70\t98/0]\t loss : 0.0854 \t accuracy : 96.68% \t495/512\n",
      "[70\t98/40]\t loss : 0.0533 \t accuracy : 98.05% \t502/512\n",
      "[70\t98/80]\t loss : 0.0846 \t accuracy : 97.07% \t497/512\n",
      "70 epoch 평균\tloss : 0.07649250028236788 \t accuracy : 96.94% \t 48642/50176\n",
      "\n",
      "\n",
      "[71\t98/0]\t loss : 0.0657 \t accuracy : 97.66% \t500/512\n",
      "[71\t98/40]\t loss : 0.0868 \t accuracy : 96.29% \t493/512\n",
      "[71\t98/80]\t loss : 0.0758 \t accuracy : 97.07% \t497/512\n",
      "71 epoch 평균\tloss : 0.08016478342517293 \t accuracy : 96.85% \t 48593/50176\n",
      "\n",
      "\n",
      "[72\t98/0]\t loss : 0.0658 \t accuracy : 97.27% \t498/512\n",
      "[72\t98/40]\t loss : 0.0662 \t accuracy : 97.85% \t501/512\n",
      "[72\t98/80]\t loss : 0.0774 \t accuracy : 96.68% \t495/512\n",
      "72 epoch 평균\tloss : 0.07194515549558766 \t accuracy : 97.10% \t 48719/50176\n",
      "\n",
      "\n",
      "[73\t98/0]\t loss : 0.0564 \t accuracy : 98.24% \t503/512\n",
      "[73\t98/40]\t loss : 0.0922 \t accuracy : 97.27% \t498/512\n",
      "[73\t98/80]\t loss : 0.0667 \t accuracy : 97.46% \t499/512\n",
      "73 epoch 평균\tloss : 0.07184022817076467 \t accuracy : 97.17% \t 48755/50176\n",
      "\n",
      "\n",
      "[74\t98/0]\t loss : 0.0870 \t accuracy : 97.46% \t499/512\n",
      "[74\t98/40]\t loss : 0.0669 \t accuracy : 98.24% \t503/512\n",
      "[74\t98/80]\t loss : 0.0549 \t accuracy : 98.05% \t502/512\n",
      "74 epoch 평균\tloss : 0.07070440099555618 \t accuracy : 97.21% \t 48778/50176\n",
      "\n",
      "\n",
      "[75\t98/0]\t loss : 0.0634 \t accuracy : 98.44% \t504/512\n",
      "[75\t98/40]\t loss : 0.0306 \t accuracy : 99.02% \t507/512\n",
      "[75\t98/80]\t loss : 0.0781 \t accuracy : 97.66% \t500/512\n",
      "75 epoch 평균\tloss : 0.06548017601729653 \t accuracy : 97.43% \t 48887/50176\n",
      "\n",
      "\n",
      "[76\t98/0]\t loss : 0.0721 \t accuracy : 97.07% \t497/512\n",
      "[76\t98/40]\t loss : 0.0822 \t accuracy : 96.09% \t492/512\n",
      "[76\t98/80]\t loss : 0.0588 \t accuracy : 97.85% \t501/512\n",
      "76 epoch 평균\tloss : 0.06979291093516714 \t accuracy : 97.13% \t 48738/50176\n",
      "\n",
      "\n",
      "[77\t98/0]\t loss : 0.0383 \t accuracy : 98.44% \t504/512\n",
      "[77\t98/40]\t loss : 0.0665 \t accuracy : 97.46% \t499/512\n",
      "[77\t98/80]\t loss : 0.0548 \t accuracy : 98.05% \t502/512\n",
      "77 epoch 평균\tloss : 0.06636219053548209 \t accuracy : 97.36% \t 48853/50176\n",
      "\n",
      "\n",
      "[78\t98/0]\t loss : 0.0559 \t accuracy : 98.05% \t502/512\n",
      "[78\t98/40]\t loss : 0.0438 \t accuracy : 98.63% \t505/512\n",
      "[78\t98/80]\t loss : 0.0505 \t accuracy : 98.05% \t502/512\n",
      "78 epoch 평균\tloss : 0.0638941916808182 \t accuracy : 97.44% \t 48892/50176\n",
      "\n",
      "\n",
      "[79\t98/0]\t loss : 0.0639 \t accuracy : 97.27% \t498/512\n",
      "[79\t98/40]\t loss : 0.0570 \t accuracy : 97.85% \t501/512\n",
      "[79\t98/80]\t loss : 0.0613 \t accuracy : 98.24% \t503/512\n",
      "79 epoch 평균\tloss : 0.06787924848648966 \t accuracy : 97.26% \t 48803/50176\n",
      "\n",
      "\n",
      "[80\t98/0]\t loss : 0.0774 \t accuracy : 97.66% \t500/512\n",
      "[80\t98/40]\t loss : 0.0487 \t accuracy : 98.24% \t503/512\n",
      "[80\t98/80]\t loss : 0.0533 \t accuracy : 97.85% \t501/512\n",
      "80 epoch 평균\tloss : 0.06532417864975881 \t accuracy : 97.42% \t 48881/50176\n",
      "\n",
      "\n",
      "[81\t98/0]\t loss : 0.0648 \t accuracy : 97.66% \t500/512\n",
      "[81\t98/40]\t loss : 0.0491 \t accuracy : 98.05% \t502/512\n",
      "[81\t98/80]\t loss : 0.0615 \t accuracy : 98.24% \t503/512\n",
      "81 epoch 평균\tloss : 0.06602584604858136 \t accuracy : 97.35% \t 48845/50176\n",
      "\n",
      "\n",
      "[82\t98/0]\t loss : 0.0453 \t accuracy : 97.85% \t501/512\n",
      "[82\t98/40]\t loss : 0.0394 \t accuracy : 99.02% \t507/512\n",
      "[82\t98/80]\t loss : 0.0425 \t accuracy : 98.24% \t503/512\n",
      "82 epoch 평균\tloss : 0.06161679373103744 \t accuracy : 97.50% \t 48921/50176\n",
      "\n",
      "\n",
      "[83\t98/0]\t loss : 0.0668 \t accuracy : 98.05% \t502/512\n",
      "[83\t98/40]\t loss : 0.0782 \t accuracy : 97.46% \t499/512\n",
      "[83\t98/80]\t loss : 0.0925 \t accuracy : 97.66% \t500/512\n",
      "83 epoch 평균\tloss : 0.061853059947642744 \t accuracy : 97.53% \t 48938/50176\n",
      "\n",
      "\n",
      "[84\t98/0]\t loss : 0.0534 \t accuracy : 98.24% \t503/512\n",
      "[84\t98/40]\t loss : 0.0564 \t accuracy : 98.24% \t503/512\n",
      "[84\t98/80]\t loss : 0.0907 \t accuracy : 96.48% \t494/512\n",
      "84 epoch 평균\tloss : 0.06267436795240763 \t accuracy : 97.47% \t 48908/50176\n",
      "\n",
      "\n",
      "[85\t98/0]\t loss : 0.0385 \t accuracy : 98.83% \t506/512\n",
      "[85\t98/40]\t loss : 0.0673 \t accuracy : 98.05% \t502/512\n",
      "[85\t98/80]\t loss : 0.0772 \t accuracy : 97.07% \t497/512\n",
      "85 epoch 평균\tloss : 0.06015056428708593 \t accuracy : 97.56% \t 48954/50176\n",
      "\n",
      "\n",
      "[86\t98/0]\t loss : 0.0574 \t accuracy : 98.05% \t502/512\n",
      "[86\t98/40]\t loss : 0.0369 \t accuracy : 98.63% \t505/512\n",
      "[86\t98/80]\t loss : 0.0504 \t accuracy : 98.44% \t504/512\n",
      "86 epoch 평균\tloss : 0.05371389453470402 \t accuracy : 97.78% \t 49063/50176\n",
      "\n",
      "\n",
      "[87\t98/0]\t loss : 0.0265 \t accuracy : 99.22% \t508/512\n",
      "[87\t98/40]\t loss : 0.0949 \t accuracy : 97.07% \t497/512\n",
      "[87\t98/80]\t loss : 0.0416 \t accuracy : 98.24% \t503/512\n",
      "87 epoch 평균\tloss : 0.058635262791447484 \t accuracy : 97.65% \t 48996/50176\n",
      "\n",
      "\n",
      "[88\t98/0]\t loss : 0.0664 \t accuracy : 97.46% \t499/512\n",
      "[88\t98/40]\t loss : 0.0664 \t accuracy : 97.85% \t501/512\n",
      "[88\t98/80]\t loss : 0.0499 \t accuracy : 98.24% \t503/512\n",
      "88 epoch 평균\tloss : 0.05684581632744902 \t accuracy : 97.63% \t 48988/50176\n",
      "\n",
      "\n",
      "[89\t98/0]\t loss : 0.0461 \t accuracy : 98.05% \t502/512\n",
      "[89\t98/40]\t loss : 0.0665 \t accuracy : 97.66% \t500/512\n",
      "[89\t98/80]\t loss : 0.0448 \t accuracy : 97.85% \t501/512\n",
      "89 epoch 평균\tloss : 0.05558088678410468 \t accuracy : 97.65% \t 48997/50176\n",
      "\n",
      "\n",
      "[90\t98/0]\t loss : 0.0540 \t accuracy : 97.85% \t501/512\n",
      "[90\t98/40]\t loss : 0.0652 \t accuracy : 98.63% \t505/512\n",
      "[90\t98/80]\t loss : 0.0480 \t accuracy : 98.63% \t505/512\n",
      "90 epoch 평균\tloss : 0.05341368219910228 \t accuracy : 97.81% \t 49079/50176\n",
      "\n",
      "\n",
      "[91\t98/0]\t loss : 0.0644 \t accuracy : 97.46% \t499/512\n",
      "[91\t98/40]\t loss : 0.0246 \t accuracy : 99.22% \t508/512\n",
      "[91\t98/80]\t loss : 0.0127 \t accuracy : 99.80% \t511/512\n",
      "91 epoch 평균\tloss : 0.05154595060312018 \t accuracy : 97.86% \t 49104/50176\n",
      "\n",
      "\n",
      "[92\t98/0]\t loss : 0.0764 \t accuracy : 97.07% \t497/512\n",
      "[92\t98/40]\t loss : 0.0448 \t accuracy : 98.83% \t506/512\n",
      "[92\t98/80]\t loss : 0.0558 \t accuracy : 98.24% \t503/512\n",
      "92 epoch 평균\tloss : 0.047092522542011385 \t accuracy : 98.02% \t 49184/50176\n",
      "\n",
      "\n",
      "[93\t98/0]\t loss : 0.0657 \t accuracy : 97.46% \t499/512\n",
      "[93\t98/40]\t loss : 0.0894 \t accuracy : 97.27% \t498/512\n",
      "[93\t98/80]\t loss : 0.0646 \t accuracy : 97.27% \t498/512\n",
      "93 epoch 평균\tloss : 0.05537936192157927 \t accuracy : 97.71% \t 49025/50176\n",
      "\n",
      "\n",
      "[94\t98/0]\t loss : 0.0534 \t accuracy : 98.24% \t503/512\n",
      "[94\t98/40]\t loss : 0.0516 \t accuracy : 97.85% \t501/512\n",
      "[94\t98/80]\t loss : 0.0489 \t accuracy : 98.05% \t502/512\n",
      "94 epoch 평균\tloss : 0.056529933187578416 \t accuracy : 97.63% \t 48988/50176\n",
      "\n",
      "\n",
      "[95\t98/0]\t loss : 0.0427 \t accuracy : 98.63% \t505/512\n",
      "[95\t98/40]\t loss : 0.0525 \t accuracy : 98.24% \t503/512\n",
      "[95\t98/80]\t loss : 0.0612 \t accuracy : 98.05% \t502/512\n",
      "95 epoch 평균\tloss : 0.05933309344536797 \t accuracy : 97.63% \t 48985/50176\n",
      "\n",
      "\n",
      "[96\t98/0]\t loss : 0.0512 \t accuracy : 98.24% \t503/512\n",
      "[96\t98/40]\t loss : 0.0268 \t accuracy : 99.02% \t507/512\n",
      "[96\t98/80]\t loss : 0.0483 \t accuracy : 97.66% \t500/512\n",
      "96 epoch 평균\tloss : 0.05348860327990687 \t accuracy : 97.74% \t 49040/50176\n",
      "\n",
      "\n",
      "[97\t98/0]\t loss : 0.0494 \t accuracy : 98.24% \t503/512\n",
      "[97\t98/40]\t loss : 0.0356 \t accuracy : 98.44% \t504/512\n",
      "[97\t98/80]\t loss : 0.0370 \t accuracy : 98.44% \t504/512\n",
      "97 epoch 평균\tloss : 0.05014600656090344 \t accuracy : 97.91% \t 49126/50176\n",
      "\n",
      "\n",
      "[98\t98/0]\t loss : 0.0740 \t accuracy : 98.05% \t502/512\n",
      "[98\t98/40]\t loss : 0.0508 \t accuracy : 97.66% \t500/512\n",
      "[98\t98/80]\t loss : 0.0613 \t accuracy : 97.46% \t499/512\n",
      "98 epoch 평균\tloss : 0.050868682164166665 \t accuracy : 97.92% \t 49134/50176\n",
      "\n",
      "\n",
      "[99\t98/0]\t loss : 0.0610 \t accuracy : 97.66% \t500/512\n",
      "[99\t98/40]\t loss : 0.0666 \t accuracy : 97.85% \t501/512\n",
      "[99\t98/80]\t loss : 0.0352 \t accuracy : 98.44% \t504/512\n",
      "99 epoch 평균\tloss : 0.048479717547948255 \t accuracy : 98.01% \t 49179/50176\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 학습\n",
    "import torch.optim as optim\n",
    "epochs = 100\n",
    "lr = 0.001\n",
    "weight_decay = 0\n",
    "\n",
    "interver=40\n",
    "\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "for epoch in range(0,epochs):\n",
    "    \n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    running_loss = 0\n",
    "    total_correct = 0\n",
    "    for i,(img,target) in enumerate(trainloader):\n",
    "        optimizer.zero_grad() # model의 gradient 값을 0으로 설정\n",
    "        \n",
    "        outputs = model(img.to(device))\n",
    "        \n",
    "        \n",
    "        loss = criterion(outputs, target.to(device))\n",
    "        loss.backward() # backward 함수를 호출해 gradient 계산\n",
    "        optimizer.step() # 모델의 학습 파라미터 갱신\n",
    "        \n",
    "        running_loss += loss.item() / len(trainloader)\n",
    "        \n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        correct = (predicted == target.to(device)).sum().item() \n",
    "        total_correct += correct\n",
    "        \n",
    "        if i % interver ==0:\n",
    "            print(f'[{epoch}\\t{len(trainloader)}/{i}]\\t loss : {loss:.4f} \\t accuracy : {correct/batch_size*100:.2f}% \\t{correct}/{batch_size}')    \n",
    "    \n",
    "\n",
    "    print(f\"{epoch} epoch 평균\\tloss : {running_loss} \\t accuracy : {total_correct/(batch_size*len(trainloader))*100:.2f}% \\t {total_correct}/{batch_size*len(trainloader)}\")\n",
    "    print(\"\\n\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 넣기전 torch.Size([2, 4097, 768])\n",
    "# 2 4097 3 12 21\n",
    "# # (배치,패치수+1,3,해더수,해더 차원)\n",
    "\n",
    "# shape '[2, 4097, 3, 12, 21]' is invalid for input of size 6292992\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "5320f61119de878fb31c813ca30206b3db486be99fef9b78f5991d1f2558cf1d"
  },
  "kernelspec": {
   "display_name": "Python 3.7.5 ('env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
