{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "[0] train loss: 1.719, validation loss: 1.388, validation acc 50.92 %\n",
      "[1] train loss: 1.393, validation loss: 1.267, validation acc 55.00 %\n",
      "[2] train loss: 1.286, validation loss: 1.266, validation acc 55.32 %\n",
      "[3] train loss: 1.208, validation loss: 1.134, validation acc 59.68 %\n",
      "[4] train loss: 1.158, validation loss: 1.134, validation acc 58.90 %\n",
      "[5] train loss: 1.118, validation loss: 1.055, validation acc 62.66 %\n",
      "[6] train loss: 1.079, validation loss: 1.034, validation acc 63.08 %\n",
      "[7] train loss: 1.048, validation loss: 1.031, validation acc 63.90 %\n",
      "[8] train loss: 1.025, validation loss: 1.007, validation acc 64.52 %\n",
      "[9] train loss: 0.996, validation loss: 0.963, validation acc 65.26 %\n",
      "[10] train loss: 0.965, validation loss: 0.955, validation acc 66.72 %\n",
      "[11] train loss: 0.954, validation loss: 0.958, validation acc 65.86 %\n",
      "[12] train loss: 0.927, validation loss: 0.941, validation acc 66.50 %\n",
      "[13] train loss: 0.913, validation loss: 0.899, validation acc 67.98 %\n",
      "[14] train loss: 0.887, validation loss: 0.898, validation acc 68.40 %\n",
      "[15] train loss: 0.873, validation loss: 0.874, validation acc 69.64 %\n",
      "[16] train loss: 0.848, validation loss: 0.890, validation acc 68.34 %\n",
      "[17] train loss: 0.844, validation loss: 0.843, validation acc 70.02 %\n",
      "[18] train loss: 0.814, validation loss: 0.859, validation acc 69.96 %\n",
      "[19] train loss: 0.803, validation loss: 0.813, validation acc 71.12 %\n",
      "[20] train loss: 0.784, validation loss: 0.837, validation acc 71.14 %\n",
      "[21] train loss: 0.764, validation loss: 0.805, validation acc 71.72 %\n",
      "[22] train loss: 0.760, validation loss: 0.824, validation acc 71.58 %\n",
      "[23] train loss: 0.736, validation loss: 0.806, validation acc 71.92 %\n",
      "[24] train loss: 0.730, validation loss: 0.795, validation acc 72.74 %\n",
      "[25] train loss: 0.716, validation loss: 0.791, validation acc 72.80 %\n",
      "[26] train loss: 0.706, validation loss: 0.775, validation acc 73.40 %\n",
      "[27] train loss: 0.689, validation loss: 0.767, validation acc 73.16 %\n",
      "[28] train loss: 0.677, validation loss: 0.802, validation acc 73.40 %\n",
      "[29] train loss: 0.664, validation loss: 0.749, validation acc 73.80 %\n",
      "[30] train loss: 0.653, validation loss: 0.783, validation acc 73.84 %\n",
      "[31] train loss: 0.641, validation loss: 0.748, validation acc 74.20 %\n",
      "[32] train loss: 0.628, validation loss: 0.730, validation acc 74.46 %\n",
      "[33] train loss: 0.618, validation loss: 0.728, validation acc 75.30 %\n",
      "[34] train loss: 0.607, validation loss: 0.739, validation acc 74.26 %\n",
      "[35] train loss: 0.597, validation loss: 0.737, validation acc 75.06 %\n",
      "[36] train loss: 0.587, validation loss: 0.741, validation acc 74.52 %\n",
      "[37] train loss: 0.570, validation loss: 0.728, validation acc 75.90 %\n",
      "[38] train loss: 0.571, validation loss: 0.699, validation acc 76.32 %\n",
      "[39] train loss: 0.548, validation loss: 0.705, validation acc 75.80 %\n",
      "[40] train loss: 0.545, validation loss: 0.713, validation acc 75.92 %\n",
      "[41] train loss: 0.536, validation loss: 0.674, validation acc 76.94 %\n",
      "[42] train loss: 0.523, validation loss: 0.663, validation acc 77.00 %\n",
      "[43] train loss: 0.514, validation loss: 0.680, validation acc 76.82 %\n",
      "[44] train loss: 0.507, validation loss: 0.675, validation acc 77.40 %\n",
      "[45] train loss: 0.503, validation loss: 0.695, validation acc 77.16 %\n",
      "[46] train loss: 0.487, validation loss: 0.691, validation acc 76.98 %\n",
      "[47] train loss: 0.483, validation loss: 0.699, validation acc 77.28 %\n",
      "[48] train loss: 0.474, validation loss: 0.697, validation acc 76.70 %\n",
      "[49] train loss: 0.460, validation loss: 0.697, validation acc 76.66 %\n",
      "[50] train loss: 0.454, validation loss: 0.716, validation acc 77.22 %\n",
      "[51] train loss: 0.452, validation loss: 0.695, validation acc 77.58 %\n",
      "[52] train loss: 0.439, validation loss: 0.686, validation acc 77.84 %\n",
      "[53] train loss: 0.434, validation loss: 0.695, validation acc 76.90 %\n",
      "[54] train loss: 0.428, validation loss: 0.701, validation acc 77.52 %\n",
      "[55] train loss: 0.417, validation loss: 0.701, validation acc 77.20 %\n",
      "[56] train loss: 0.412, validation loss: 0.684, validation acc 78.18 %\n",
      "[57] train loss: 0.402, validation loss: 0.736, validation acc 77.28 %\n",
      "[58] train loss: 0.398, validation loss: 0.721, validation acc 77.02 %\n",
      "[59] train loss: 0.389, validation loss: 0.738, validation acc 77.16 %\n",
      "[60] train loss: 0.382, validation loss: 0.722, validation acc 77.72 %\n",
      "[61] train loss: 0.381, validation loss: 0.699, validation acc 78.04 %\n",
      "[62] train loss: 0.372, validation loss: 0.727, validation acc 77.86 %\n",
      "[63] train loss: 0.365, validation loss: 0.714, validation acc 78.16 %\n",
      "[64] train loss: 0.354, validation loss: 0.765, validation acc 76.92 %\n",
      "[65] train loss: 0.356, validation loss: 0.705, validation acc 78.16 %\n",
      "[66] train loss: 0.351, validation loss: 0.743, validation acc 77.70 %\n",
      "[67] train loss: 0.348, validation loss: 0.726, validation acc 77.36 %\n",
      "[68] train loss: 0.340, validation loss: 0.733, validation acc 78.10 %\n",
      "[69] train loss: 0.326, validation loss: 0.735, validation acc 77.88 %\n",
      "[70] train loss: 0.331, validation loss: 0.741, validation acc 77.72 %\n",
      "[71] train loss: 0.319, validation loss: 0.711, validation acc 78.58 %\n",
      "[72] train loss: 0.320, validation loss: 0.749, validation acc 77.86 %\n",
      "[73] train loss: 0.313, validation loss: 0.751, validation acc 78.30 %\n",
      "[74] train loss: 0.307, validation loss: 0.755, validation acc 78.38 %\n",
      "[75] train loss: 0.307, validation loss: 0.750, validation acc 78.06 %\n",
      "[76] train loss: 0.292, validation loss: 0.755, validation acc 77.32 %\n",
      "[77] train loss: 0.289, validation loss: 0.764, validation acc 77.00 %\n",
      "[78] train loss: 0.292, validation loss: 0.773, validation acc 78.04 %\n",
      "[79] train loss: 0.277, validation loss: 0.783, validation acc 77.58 %\n",
      "[80] train loss: 0.276, validation loss: 0.786, validation acc 78.06 %\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_18003/4234744408.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     63\u001b[0m                 \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m                 \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m                 \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m                 \u001b[0mrunning_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m                 \u001b[0;31m#scheduler.step()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data/mrjaehong/handwriting_gen/pytorch_vit/env/lib/python3.7/site-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     86\u001b[0m                 \u001b[0mprofile_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Optimizer.step#{}.step\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecord_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprofile_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data/mrjaehong/handwriting_gen/pytorch_vit/env/lib/python3.7/site-packages/torch/autograd/grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data/mrjaehong/handwriting_gen/pytorch_vit/env/lib/python3.7/site-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    151\u001b[0m                    \u001b[0mweight_decay\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'weight_decay'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m                    \u001b[0meps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'eps'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 153\u001b[0;31m                    maximize=group['maximize'])\n\u001b[0m\u001b[1;32m    154\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data/mrjaehong/handwriting_gen/pytorch_vit/env/lib/python3.7/site-packages/torch/optim/_functional.py\u001b[0m in \u001b[0;36madam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[1;32m    103\u001b[0m             \u001b[0mdenom\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmax_exp_avg_sqs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias_correction2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 105\u001b[0;31m             \u001b[0mdenom\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mexp_avg_sq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias_correction2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    106\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import patchdata\n",
    "import model\n",
    "import test\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    img_size = 32\n",
    "    patch_size = 4\n",
    "    batch_size = 128\n",
    "    save_acc =50\n",
    "    epochs = 500\n",
    "    lr = 0.001\n",
    "    drop_rate = .1\n",
    "    weight_decay = 0\n",
    "    num_classes = 10\n",
    "    latent_vec_dim = 128\n",
    "    num_heads = 8\n",
    "    num_layers = 12\n",
    "    dataname = 'cifar10'\n",
    "    mode = 'train'\n",
    "    pretrained = 0\n",
    "\n",
    "\n",
    "\n",
    "    latent_vec_dim = latent_vec_dim\n",
    "    mlp_hidden_dim = int(latent_vec_dim/2)\n",
    "    num_patches = int((img_size * img_size) / (patch_size * patch_size))\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # Image Patches\n",
    "    d = patchdata.Flattened2Dpatches(dataname=dataname, img_size=img_size, patch_size=patch_size,\n",
    "                                     batch_size=batch_size)\n",
    "    trainloader, valloader, testloader = d.patchdata()\n",
    "    image_patches, _ = iter(trainloader).next()\n",
    "\n",
    "    \n",
    "    # Model\n",
    "    vit = model.VisionTransformer(patch_vec_size=image_patches.size(2), num_patches=image_patches.size(1),\n",
    "                                  latent_vec_dim=latent_vec_dim, num_heads=num_heads, mlp_hidden_dim=mlp_hidden_dim,\n",
    "                                  drop_rate=drop_rate, num_layers=num_layers, num_classes=num_classes).to(device)\n",
    "\n",
    "    if pretrained == 1:\n",
    "        vit.load_state_dict(torch.load('./model.pth'))\n",
    "\n",
    "    if mode == 'train':\n",
    "        # Loss and optimizer\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        optimizer = optim.Adam(vit.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "        #optimizer = torch.optim.SGD(vit.parameters(), lr=args.lr, momentum=0.9)\n",
    "        #scheduler = optim.lr_scheduler.OneCycleLR(optimizer, max_lr=args.lr, steps_per_epoch=len(trainloader), epochs=args.epochs)\n",
    "\n",
    "        # Train\n",
    "        n = len(trainloader)\n",
    "        best_acc = save_acc\n",
    "        for epoch in range(epochs):\n",
    "            running_loss = 0\n",
    "            for img, labels in trainloader:\n",
    "                optimizer.zero_grad()\n",
    "                outputs, _ = vit(img.to(device))\n",
    "                loss = criterion(outputs, labels.to(device))\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                running_loss += loss.item()\n",
    "                #scheduler.step()\n",
    "\n",
    "            train_loss = running_loss / n\n",
    "            val_acc, val_loss = test.accuracy(valloader, vit)\n",
    "            # if epoch % 5 == 0:\n",
    "            print('[%d] train loss: %.3f, validation loss: %.3f, validation acc %.2f %%' % (epoch, train_loss, val_loss, val_acc))\n",
    "\n",
    "            if val_acc > best_acc:\n",
    "                best_acc = val_acc\n",
    "                # print('[%d] train loss: %.3f, validation acc %.2f - Save the best model' % (epoch, train_loss, val_acc))\n",
    "                torch.save(vit.state_dict(), './model.pth')\n",
    "\n",
    "    else:\n",
    "        test_acc, test_loss = test.accuracy(testloader, vit)\n",
    "        print('test loss: %.3f, test acc %.2f %%' % (test_loss, test_acc))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "5320f61119de878fb31c813ca30206b3db486be99fef9b78f5991d1f2558cf1d"
  },
  "kernelspec": {
   "display_name": "Python 3.7.5 ('env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
